{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8943aaf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "# python core imports\n",
    "from venv import create\n",
    "from typing import Callable, Dict, Optional, Tuple\n",
    "from abc import abstractmethod\n",
    "import struct\n",
    "import os\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import time\n",
    "import math\n",
    "from re import M\n",
    "import shutil\n",
    "import argparse\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "# python external modules\n",
    "import scipy.io\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import tqdm\n",
    "import pandas as pd\n",
    "from posixpath import split\n",
    "\n",
    "# torch imports\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.cuda import amp\n",
    "from torchvision.datasets import DatasetFolder, utils\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# spikingjelly imports\n",
    "from spikingjelly.clock_driven import functional, surrogate, layer, neuron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "74221b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config variables\n",
    "np_savez = np.savez_compressed\n",
    "_seed_ = 2020\n",
    "torch.manual_seed(_seed_)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(_seed_)\n",
    "\n",
    "# logging setup for production only\n",
    "# logging.basicConfig(filename=f'logs/runlog.log', level=logging.DEBUG, format='%(asctime)s:%(levelname)s:%(message)s')\n",
    "# logger = logging.getLogger()\n",
    "# sys.stderr.write = logger.error\n",
    "# sys.stdout.write = logger.info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1d37db7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_frame(x: torch.Tensor or np.ndarray, save_gif_to: str = None) -> None:\n",
    "    '''\n",
    "    :param x: frames with ``shape=[T, 2, H, W]``\n",
    "    :type x: torch.Tensor or np.ndarray\n",
    "    :param save_gif_to: If ``None``, this function will play the frames. If ``True``, this function will not play the frames\n",
    "        but save frames to a gif file in the directory ``save_gif_to``\n",
    "    :type save_gif_to: str\n",
    "    :return: None\n",
    "    '''\n",
    "\n",
    "    if isinstance(x, np.ndarray):\n",
    "        x = torch.from_numpy(x)\n",
    "    \n",
    "    to_img = transforms.ToPILImage()\n",
    "    img_tensor = torch.zeros([x.shape[0], 3, x.shape[2], x.shape[3]])\n",
    "    img_tensor[:, 1] = x[:, 0]\n",
    "    img_tensor[:, 2] = x[:, 1]\n",
    "    if save_gif_to is None:\n",
    "        while True:\n",
    "            for t in range(img_tensor.shape[0]):\n",
    "                    plt.imshow(to_img(img_tensor[t]))\n",
    "                    plt.pause(0.01)\n",
    "    else:\n",
    "        img_list = []\n",
    "        for t in range(img_tensor.shape[0]):\n",
    "            img_list.append(to_img(img_tensor[t]))\n",
    "        img_list[0].save(save_gif_to, save_all=True, append_images=img_list[1:], loop=0)\n",
    "        print(f'Save frames to [{save_gif_to}].')\n",
    "\n",
    "\n",
    "def load_matlab_mat(file_name: str) -> Dict:\n",
    "    '''\n",
    "    :param file_name: path of the matlab's mat file\n",
    "    :type file_name: str\n",
    "    :return: a dict whose keys are ``['x', 'y', 'p', 't']`` and values are ``numpy.ndarray``\n",
    "    :rtype: Dict\n",
    "    '''\n",
    "    events = scipy.io.loadmat(file_name)\n",
    "    return {\n",
    "        'x': events['x'].squeeze(),\n",
    "        'y': events['y'].squeeze(),\n",
    "        'p': events['pol'].squeeze(),\n",
    "        't': events['ts'].squeeze()\n",
    "    }\n",
    "\n",
    "\n",
    "def load_raw(file_name: str) -> Dict:\n",
    "    '''\n",
    "    :param file_name: path of the raw file\n",
    "    :type file_name: str\n",
    "    :return: a dict whose keys are ``['x', 'y', 'p', 't']`` and values are ``numpy.ndarray``\n",
    "    :rtype: Dict\n",
    "    '''\n",
    "\n",
    "    df = pd.read_csv(f'./data/{file_name}', names=['x', 'y', 'p', 't'], sep=',')\n",
    "\n",
    "    return {\n",
    "        'x': np.array(df['x']),\n",
    "        'y': np.array(df['y']),\n",
    "        'p': np.array(df['p']),\n",
    "        't': np.array(df['t'])\n",
    "    }\n",
    "\n",
    "\n",
    "def load_npz_frames(file_name: str) -> np.ndarray:\n",
    "    '''\n",
    "    :param file_name: path of the npz file that saves the frames\n",
    "    :type file_name: str\n",
    "    :return: frames\n",
    "    :rtype: np.ndarray\n",
    "    '''\n",
    "\n",
    "    return np.load(file_name, allow_pickle=True)['frames']\n",
    "\n",
    "\n",
    "def integrate_events_segment_to_frame(events: Dict, H: int, W: int, j_l: int=0, j_r: int = -1) -> np.ndarray:\n",
    "    '''\n",
    "    :param events: a dict whose keys are ``['x', 'y', 'p', 't']`` and values are ``numpy.ndarray``\n",
    "    :type events: Dict\n",
    "    :param H: height of the frame\n",
    "    :type H: int\n",
    "    :param W: weight of the frame\n",
    "    :type W: int\n",
    "    :param j_l: the start index of the integral interval, which is included\n",
    "    :type j_l: int\n",
    "    :param j_r: the right index of the integral interval, which is not included\n",
    "    :type j_r:\n",
    "    :return: frames\n",
    "    :rtype: np.ndarray\n",
    "    Denote a two channels frame as :math:`F` and a pixel at :math:`(p, x, y)` as :math:`F(p, x, y)`, the pixel value is integrated from the events data whose indices are in :math:`[j_{l}, j_{r})`:\n",
    "    .. math::\n",
    "        F(p, x, y) = \\sum_{i = j_{l}}^{j_{r} - 1} \\mathcal{I}_{p, x, y}(p_{i}, x_{i}, y_{i})\n",
    "    where :math:`\\lfloor \\cdot \\rfloor` is the floor operation, :math:`\\mathcal{I}_{p, x, y}(p_{i}, x_{i}, y_{i})` is an indicator function and it equals 1 only when :math:`(p, x, y) = (p_{i}, x_{i}, y_{i})`.\n",
    "    '''\n",
    "\n",
    "    frame = np.zeros(shape=[2, H*W])\n",
    "    x = events['x'][j_l: j_r].astype(int)\n",
    "    y = events['y'][j_l: j_r].astype(int)\n",
    "    p = events['p'][j_l: j_r]\n",
    "    mask = []\n",
    "    mask.append(p==0)\n",
    "    mask.append(np.logical_not(mask[0]))\n",
    "\n",
    "    for c in range(2):\n",
    "        position = y[mask[c]] * W + x[mask[c]]\n",
    "        events_number_per_pos = np.bincount(position)\n",
    "        frame[c][np.arange(events_number_per_pos.size)] += events_number_per_pos\n",
    "    \n",
    "    return frame.reshape((2, H, W))\n",
    "\n",
    "\n",
    "def cal_fixed_frames_number_segment_index(events_t: np.ndarray, split_by: str, frames_num: int) -> tuple:\n",
    "    '''\n",
    "    :param events_t: events' t\n",
    "    :type events_t: numpy.ndarray\n",
    "    :param split_by: 'time' or 'number'\n",
    "    :type split_by: str\n",
    "    :param frames_num: the number of frames\n",
    "    :type frames_num: int\n",
    "    :return: a tuple ``(j_l, j_r)``\n",
    "    :rtype: tuple\n",
    "    Denote ``frames_num`` as :math:`M`, if ``split_by`` is ``'time'``, then\n",
    "    .. math::\n",
    "        \\\\Delta T & = [\\\\frac{t_{N-1} - t_{0}}{M}] \\\\\\\\\n",
    "        j_{l} & = \\\\mathop{\\\\arg\\\\min}\\\\limits_{k} \\\\{t_{k} | t_{k} \\\\geq t_{0} + \\\\Delta T \\\\cdot j\\\\} \\\\\\\\\n",
    "        j_{r} & = \\\\begin{cases} \\\\mathop{\\\\arg\\\\max}\\\\limits_{k} \\\\{t_{k} | t_{k} < t_{0} + \\\\Delta T \\\\cdot (j + 1)\\\\} + 1, & j <  M - 1 \\\\cr N, & j = M - 1 \\\\end{cases}\n",
    "    If ``split_by`` is ``'number'``, then\n",
    "    .. math::\n",
    "        j_{l} & = [\\\\frac{N}{M}] \\\\cdot j \\\\\\\\\n",
    "        j_{r} & = \\\\begin{cases} [\\\\frac{N}{M}] \\\\cdot (j + 1), & j <  M - 1 \\\\cr N, & j = M - 1 \\\\end{cases}\n",
    "    '''\n",
    "\n",
    "    j_l = np.zeros(shape=[frames_num], dtype=int)\n",
    "    j_r = np.zeros(shape=[frames_num], dtype=int)\n",
    "    N = events_t.size\n",
    "\n",
    "    if split_by == 'number':\n",
    "        di = N // frames_num\n",
    "        for i in range(frames_num):\n",
    "            j_l[i] = i * di\n",
    "            j_r[i] = j_l[i] + di\n",
    "        j_r[-1] = N\n",
    "\n",
    "    elif split_by == 'time':\n",
    "        dt = (events_t[-1] - events_t[0]) // frames_num\n",
    "        idx = np.arange(N)\n",
    "        for i in range(frames_num):\n",
    "            t_l = dt * i + events_t[0]\n",
    "            t_r = t_l + dt\n",
    "            mask = np.logical_and(events_t >= t_l, events_t < t_r)\n",
    "            idx_masked = idx[mask]\n",
    "            j_l[i] = idx_masked[0]\n",
    "            j_r[i] = idx_masked[-1] + 1\n",
    "\n",
    "        j_r[-1] = N\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    return j_l, j_r\n",
    "\n",
    "\n",
    "def integrate_events_by_fixed_frames_number(events: Dict, split_by: str, frames_num: int, H: int, W: int) -> np.ndarray:\n",
    "    '''\n",
    "    :param events: a dict whose keys are ``['x', 'y', 'p', 't']`` and values are ``numpy.ndarray``\n",
    "    :type events: Dict\n",
    "    :param split_by: 'time' or 'number'\n",
    "    :type split_by: str\n",
    "    :param frames_num: the number of frames\n",
    "    :type frames_num: int\n",
    "    :param H: the height of frame\n",
    "    :type H: int\n",
    "    :param W: the weight of frame\n",
    "    :type W: int\n",
    "    :return: frames\n",
    "    :rtype: np.ndarray\n",
    "    Integrate events to frames by fixed frames number. See :class:`cal_fixed_frames_number_segment_index` and :class:`integrate_events_segment_to_frame` for more details.\n",
    "    '''\n",
    "    j_l, j_r = cal_fixed_frames_number_segment_index(events['t'], split_by, frames_num)\n",
    "    frames = np.zeros([frames_num, 2, H, W])\n",
    "    for i in range(frames_num):\n",
    "        frames[i] = integrate_events_segment_to_frame(events, H, W, j_l[i], j_r[i])\n",
    "    return frames\n",
    "\n",
    "\n",
    "def integrate_events_file_to_frames_file_by_fixed_frames_number(loader: Callable, events_np_file: str, output_dir: str, split_by: str, frames_num: int, H: int, W: int, print_save: bool = False) -> None:\n",
    "    '''\n",
    "    :param loader: a function that can load events from `events_np_file`\n",
    "    :type loader: Callable\n",
    "    :param events_np_file: path of the events np file\n",
    "    :type events_np_file: str\n",
    "    :param output_dir: output directory for saving the frames\n",
    "    :type output_dir: str\n",
    "    :param split_by: 'time' or 'number'\n",
    "    :type split_by: str\n",
    "    :param frames_num: the number of frames\n",
    "    :type frames_num: int\n",
    "    :param H: the height of frame\n",
    "    :type H: int\n",
    "    :param W: the weight of frame\n",
    "    :type W: int\n",
    "    :param print_save: If ``True``, this function will print saved files' paths.\n",
    "    :type print_save: bool\n",
    "    :return: None\n",
    "    Integrate a events file to frames by fixed frames number and save it. See :class:`cal_fixed_frames_number_segment_index` and :class:`integrate_events_segment_to_frame` for more details.\n",
    "    '''\n",
    "    fname = os.path.join(output_dir, os.path.basename(events_np_file))\n",
    "    np_savez(fname, frames=integrate_events_by_fixed_frames_number(loader(events_np_file), split_by, frames_num, H, W))\n",
    "    if print_save:\n",
    "        print(f'Frames [{fname}] saved.')\n",
    "\n",
    "\n",
    "def integrate_events_by_fixed_duration(events: Dict, duration: int, H: int, W: int) -> np.ndarray:\n",
    "    '''\n",
    "    :param events: a dict whose keys are ``['x', 'y', 'p', 't']`` and values are ``numpy.ndarray``\n",
    "    :type events: Dict\n",
    "    :param duration: the time duration of each frame\n",
    "    :type duration: int\n",
    "    :param H: the height of frame\n",
    "    :type H: int\n",
    "    :param W: the weight of frame\n",
    "    :type W: int\n",
    "    :return: frames\n",
    "    :rtype: np.ndarray\n",
    "    Integrate events to frames by fixed time duration of each frame.\n",
    "    '''\n",
    "\n",
    "    t = events['t']\n",
    "    N = t.size\n",
    "\n",
    "    frames = []\n",
    "    left = 0\n",
    "    right = 0\n",
    "    while True:\n",
    "        t_l = t[left]\n",
    "        while True:\n",
    "            if right == N or t[right] - t_l > duration:\n",
    "                break\n",
    "            else:\n",
    "                right += 1\n",
    "        # integrate from index [left, right)\n",
    "        frames.append(np.expand_dims(integrate_events_segment_to_frame(events, H, W, left, right), 0))\n",
    "\n",
    "        left = right\n",
    "\n",
    "        if right == N:\n",
    "            return np.concatenate(frames)\n",
    "\n",
    "def integrate_events_file_to_frames_file_by_fixed_duration(loader: Callable, events_np_file: str, output_dir: str, duration: int, H: int, W: int, print_save: bool = False) -> None:\n",
    "    '''\n",
    "    :param loader: a function that can load events from `events_np_file`\n",
    "    :type loader: Callable\n",
    "    :param events_np_file: path of the events np file\n",
    "    :type events_np_file: str\n",
    "    :param output_dir: output directory for saving the frames\n",
    "    :type output_dir: str\n",
    "    :param duration: the time duration of each frame\n",
    "    :type duration: int\n",
    "    :param H: the height of frame\n",
    "    :type H: int\n",
    "    :param W: the weight of frame\n",
    "    :type W: int\n",
    "    :param print_save: If ``True``, this function will print saved files' paths.\n",
    "    :type print_save: bool\n",
    "    :return: None\n",
    "    Integrate events to frames by fixed time duration of each frame.\n",
    "    '''\n",
    "\n",
    "    frames = integrate_events_by_fixed_duration(loader(events_np_file), duration, H, W)\n",
    "    fname, _ = os.path.splitext(os.path.basename(events_np_file))\n",
    "    fname = os.path.join(output_dir, f'{fname}_{frames.shape[0]}.npz')\n",
    "    np_savez(fname, frames=frames)\n",
    "    if print_save:\n",
    "        print(f'Frames [{fname}] saved.')\n",
    "    return frames.shape[0]\n",
    "\n",
    "def save_frames_to_npz_and_print(fname: str, frames):\n",
    "    np_savez(fname, frames=frames)\n",
    "    print(f'Frames [{fname}] saved.')\n",
    "\n",
    "def create_same_directory_structure(source_dir: str, target_dir: str) -> None:\n",
    "    '''\n",
    "    :param source_dir: Path of the directory that be copied from\n",
    "    :type source_dir: str\n",
    "    :param target_dir: Path of the directory that be copied to\n",
    "    :type target_dir: str\n",
    "    :return: None\n",
    "    Create the same directory structure in ``target_dir`` with that of ``source_dir``.\n",
    "    '''\n",
    "    for sub_dir_name in os.listdir(source_dir):\n",
    "        source_sub_dir = os.path.join(source_dir, sub_dir_name)\n",
    "        if os.path.isdir(source_sub_dir):\n",
    "            target_sub_dir = os.path.join(target_dir, sub_dir_name)\n",
    "            os.mkdir(target_sub_dir)\n",
    "            print(f'Mkdir [{target_sub_dir}].')\n",
    "            create_same_directory_structure(source_sub_dir, target_sub_dir)\n",
    "\n",
    "\n",
    "def split_to_train_test_set(train_ratio: float, origin_dataset: torch.utils.data.Dataset, num_classes: int, random_split: bool = False):\n",
    "    '''\n",
    "    :param train_ratio: split the ratio of the origin dataset as the train set\n",
    "    :type train_ratio: float\n",
    "    :param origin_dataset: the origin dataset\n",
    "    :type origin_dataset: torch.utils.data.Dataset\n",
    "    :param num_classes: total classes number, e.g., ``10`` for the MNIST dataset\n",
    "    :type num_classes: int\n",
    "    :param random_split: If ``False``, the front ratio of samples in each classes will\n",
    "            be included in train set, while the reset will be included in test set.\n",
    "            If ``True``, this function will split samples in each classes randomly. The randomness is controlled by\n",
    "            ``numpy.random.seed``\n",
    "    :type random_split: int\n",
    "    :return: a tuple ``(train_set, test_set)``\n",
    "    :rtype: tuple\n",
    "    '''\n",
    "\n",
    "    label_idx = []\n",
    "    for i in range(num_classes):\n",
    "        label_idx.append([])\n",
    "\n",
    "    for i, item in enumerate(tqdm.tqdm(origin_dataset)):\n",
    "        y = item[1]\n",
    "        if isinstance(y, np.ndarray) or isinstance(y, torch.Tensor):\n",
    "            y = y.item()\n",
    "        \n",
    "        label_idx[y].append(i)\n",
    "    \n",
    "    train_idx = []\n",
    "    test_idx = []\n",
    "\n",
    "    if random_split:\n",
    "        for i in range(num_classes):\n",
    "            np.random.shuffle(label_idx[i])\n",
    "    \n",
    "    for i in range(num_classes):\n",
    "        pos = math.ceil(label_idx[i].__len__() * train_ratio)\n",
    "        train_idx.extend(label_idx[i][0: pos])\n",
    "        test_idx.extend(label_idx[i][pos: label_idx[i].__len__()])\n",
    "\n",
    "    return torch.utils.data.Subset(origin_dataset, train_idx), torch.utils.data.Subset(origin_dataset, test_idx)\n",
    "\n",
    "\n",
    "def pad_sequence_collate(batch: list):\n",
    "    '''\n",
    "    :param batch: a list of samples that contains ``(x, y)``, where ``x.shape=[T, *]`` and ``y`` is the label\n",
    "    :type batch: list\n",
    "    :return: batched samples, where ``x`` is padded with the same length\n",
    "    :rtype: tuple\n",
    "    This function can be use as the ``collate_fn`` for ``DataLoader`` to process the dataset with variable length, e.g., a ``NeuromorphicDatasetFolder`` with fixed duration to integrate events to frames.\n",
    "    Here is an example:\n",
    "    .. code-block:: python\n",
    "        class RandomLengthDataset(torch.utils.data.Dataset):\n",
    "            def __init__(self, n=1000):\n",
    "                super().__init__()\n",
    "                self.n = n\n",
    "            def __getitem__(self, i):\n",
    "                return torch.rand([random.randint(1, 10), 28, 28]), random.randint(0, 10)\n",
    "            def __len__(self):\n",
    "                return self.n\n",
    "        loader = torch.utils.data.DataLoader(RandomLengthDataset(n=32), batch_size=16, collate_fn=pad_sequence_collate)\n",
    "        for x, y, z in loader:\n",
    "            print(x.shape, y.shape, z)\n",
    "    And the outputs are:\n",
    "    .. code-block:: bash\n",
    "        torch.Size([10, 16, 28, 28]) torch.Size([16]) tensor([ 1,  9,  3,  4,  1,  2,  9,  7,  2,  1,  5,  7,  4, 10,  9,  5])\n",
    "        torch.Size([10, 16, 28, 28]) torch.Size([16]) tensor([ 1,  8,  7, 10,  3, 10,  6,  7,  5,  9, 10,  5,  9,  6,  7,  6])\n",
    "    '''\n",
    "    x_list = []\n",
    "    x_len_list = []\n",
    "    y_list = []\n",
    "    for x, y in batch:\n",
    "        x_list.append(torch.as_tensor(x))\n",
    "        x_len_list.append(x.shape[0])\n",
    "        y_list.append(y)\n",
    "\n",
    "    return torch.nn.utils.rnn.pad_sequence(x_list, batch_first=True), torch.as_tensor(y_list), torch.as_tensor(x_len_list)\n",
    "\n",
    "\n",
    "def padded_sequence_mask(sequence_len: torch.Tensor, T=None):\n",
    "    '''\n",
    "    :param sequence_len: a tensor ``shape = [N]`` that contains sequences lengths of each batch element\n",
    "    :type sequence_len: torch.Tensor\n",
    "    :param T: The maximum length of sequences. If ``None``, the maximum element in ``sequence_len`` will be seen as ``T``\n",
    "    :type T: int\n",
    "    :return: a bool mask with shape = [T, N], where the padded position is ``False``\n",
    "    :rtype: torch.Tensor\n",
    "    Here is an example:\n",
    "    .. code-block:: python\n",
    "        x1 = torch.rand([2, 6])\n",
    "        x2 = torch.rand([3, 6])\n",
    "        x3 = torch.rand([4, 6])\n",
    "        x = torch.nn.utils.rnn.pad_sequence([x1, x2, x3])  # [T, N, *]\n",
    "        print('x.shape=', x.shape)\n",
    "        x_len = torch.as_tensor([x1.shape[0], x2.shape[0], x3.shape[0]])\n",
    "        mask = padded_sequence_mask(x_len)\n",
    "        print('mask.shape=', mask.shape)\n",
    "        print('mask=\\\\n', mask)\n",
    "    And the outputs are:\n",
    "    .. code-block:: bash\n",
    "        x.shape= torch.Size([4, 3, 6])\n",
    "        mask.shape= torch.Size([4, 3])\n",
    "        mask=\n",
    "         tensor([[ True,  True,  True],\n",
    "                [ True,  True,  True],\n",
    "                [False,  True,  True],\n",
    "                [False, False,  True]])\n",
    "    '''\n",
    "    if T is None:\n",
    "        T = sequence_len.max().item()\n",
    "    N = sequence_len.numel()\n",
    "    device_id = sequence_len.get_device()\n",
    "\n",
    "    if device_id >= 0 and cupy is not None:\n",
    "        mask = torch.zeros([T, N], dtype=bool, device=sequence_len.device)\n",
    "        with cupy.cuda.Device(device_id):\n",
    "            T = cupy.asarray(T)\n",
    "            N = cupy.asarray(N)\n",
    "            sequence_len, mask, T, N = cu_kernel_opt.get_contiguous(sequence_len.to(torch.int), mask, T, N)\n",
    "            kernel_args = [sequence_len, mask, T, N]\n",
    "            kernel = cupy.RawKernel(padded_sequence_mask_kernel_code, 'padded_sequence_mask_kernel', options=('-use_fast_math',), backend=('-use_fast_math',))\n",
    "            blocks = cu_kernel_opt.cal_blocks(N)\n",
    "            kernel(\n",
    "                (blocks,), (1024,),\n",
    "                cu_kernel_opt.wrap_args_to_raw_kernel(\n",
    "                    device_id,\n",
    "                    *kernel_args\n",
    "                )\n",
    "            )\n",
    "            return mask\n",
    "\n",
    "    else:\n",
    "        t_seq = torch.arange(0, T).unsqueeze(1).repeat(1, N).to(sequence_len)  # [T, N]\n",
    "        return t_seq < sequence_len.unsqueeze(0).repeat(T, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "31c69417",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuromorphicDatasetFolder(DatasetFolder):\n",
    "    def __init__(\n",
    "            self,\n",
    "            root: str,\n",
    "            train: bool = None,\n",
    "            data_type: str = 'event',\n",
    "            frames_number: int = None,\n",
    "            split_by: str = None,\n",
    "            duration: int = None,\n",
    "            custom_integrate_function: Callable = None,\n",
    "            custom_integrated_frames_dir_name: str = None,\n",
    "            transform: Optional[Callable] = None,\n",
    "            target_transform: Optional[Callable] = None,\n",
    "    ) -> None:\n",
    "        '''\n",
    "        :param root: root path of the dataset\n",
    "        :type root: str\n",
    "        :param train: whether use the train set. Set ``True`` or ``False`` for those datasets provide train/test\n",
    "            division, e.g., DVS128 Gesture dataset. If the dataset does not provide train/test division, e.g., CIFAR10-DVS,\n",
    "            please set ``None`` and use :class:`~split_to_train_test_set` function to get train/test set\n",
    "        :type train: bool\n",
    "        :param data_type: `event` or `frame`\n",
    "        :type data_type: str\n",
    "        :param frames_number: the integrated frame number\n",
    "        :type frames_number: int\n",
    "        :param split_by: `time` or `number`\n",
    "        :type split_by: str\n",
    "        :param duration: the time duration of each frame\n",
    "        :type duration: int\n",
    "        :param custom_integrate_function: a user-defined function that inputs are ``events, H, W``.\n",
    "            ``events`` is a dict whose keys are ``['t', 'x', 'y', 'p']`` and values are ``numpy.ndarray``\n",
    "            ``H`` is the height of the data and ``W`` is the weight of the data.\n",
    "            For example, H=128 and W=128 for the DVS128 Gesture dataset.\n",
    "            The user should define how to integrate events to frames, and return frames.\n",
    "        :type custom_integrate_function: Callable\n",
    "        :param custom_integrated_frames_dir_name: The name of directory for saving frames integrating by ``custom_integrate_function``.\n",
    "            If ``custom_integrated_frames_dir_name`` is ``None``, it will be set to ``custom_integrate_function.__name__``\n",
    "        :type custom_integrated_frames_dir_name: str or None\n",
    "        :param transform: a function/transform that takes in\n",
    "            a sample and returns a transformed version.\n",
    "            E.g, ``transforms.RandomCrop`` for images.\n",
    "        :type transform: callable\n",
    "        :param target_transform: a function/transform that takes\n",
    "            in the target and transforms it.\n",
    "        :type target_transform: callable\n",
    "        The base class for neuromorphic dataset. Users can define a new dataset by inheriting this class and implementing\n",
    "        all abstract methods. Users can refer to :class:`spikingjelly.datasets.dvs128_gesture.DVS128Gesture`.\n",
    "        If ``data_type == 'event'``\n",
    "            the sample in this dataset is a dict whose keys are ``['t', 'x', 'y', 'p']`` and values are ``numpy.ndarray``.\n",
    "        If ``data_type == 'frame'`` and ``frames_number`` is not ``None``\n",
    "            events will be integrated to frames with fixed frames number. ``split_by`` will define how to split events.\n",
    "            See :class:`cal_fixed_frames_number_segment_index` for\n",
    "            more details.\n",
    "        If ``data_type == 'frame'``, ``frames_number`` is ``None``, and ``duration`` is not ``None``\n",
    "            events will be integrated to frames with fixed time duration.\n",
    "        If ``data_type == 'frame'``, ``frames_number`` is ``None``, ``duration`` is ``None``, and ``custom_integrate_function`` is not ``None``:\n",
    "            events will be integrated by the user-defined function and saved to the ``custom_integrated_frames_dir_name`` directory in ``root`` directory.\n",
    "        '''\n",
    "\n",
    "        events_np_root = os.path.join(root, 'events_np')\n",
    "        extract_root = os.path.join(root, 'data')\n",
    "\n",
    "        if not os.path.exists(events_np_root):\n",
    "            os.mkdir(events_np_root)\n",
    "            print(f'Mkdir [{events_np_root}]')\n",
    "        print(f'Start to convert the origin data from [{extract_root}] to [{events_np_root}] in np.ndarray format.')\n",
    "        self.create_events_np_files(extract_root, events_np_root)\n",
    "    \n",
    "        H, W = self.get_H_W()\n",
    "\n",
    "        if data_type == 'event':\n",
    "            _root = events_np_root\n",
    "            _loader = np.load\n",
    "            _transform = transform\n",
    "            _target_transform = target_transform\n",
    "\n",
    "        elif data_type == 'frame':\n",
    "            if frames_number is not None:\n",
    "                assert frames_number > 0 and isinstance(frames_number, int)\n",
    "                assert split_by == 'time' or split_by == 'number'\n",
    "\n",
    "                frames_np_root = os.path.join(root, f'frames_number_{frames_number}_split_by_{split_by}')\n",
    "\n",
    "                if os.path.exists(frames_np_root):\n",
    "                    print(f'The directory {frames_np_root} already exists')\n",
    "                else:\n",
    "                    os.mkdir(frames_np_root)\n",
    "                    print(f'Mkdir [{frames_np_root}]')\n",
    "\n",
    "                    create_same_directory_structure(events_np_root, frames_np_root)\n",
    "\n",
    "                    t_ckp = time.time()\n",
    "\n",
    "                    for e_root, e_dirs, e_files in os.walk(events_np_root):\n",
    "                        if e_files.__len__() > 0:\n",
    "                            output_dir = os.path.join(frames_np_root, os.path.relpath(e_root, events_np_root))\n",
    "                            for e_file in e_files:\n",
    "                                events_np_file = os.path.join(e_root, e_file)\n",
    "                                print(f'Start to integrate [{events_np_file}] to frames and save to [{output_dir}].')\n",
    "                                # tpe.submit(integrate_events_file_to_frames_file_by_fixed_frames_number, self.load_events_np, events_np_file, output_dir, split_by, frames_number, H, W, True)\n",
    "                                integrate_events_file_to_frames_file_by_fixed_frames_number(self.load_events_np, events_np_file, output_dir, split_by, frames_number, H, W, True)\n",
    "\n",
    "                    print(f'Used time = [{round(time.time() - t_ckp, 2)}s].')\n",
    "                \n",
    "                _root = frames_np_root\n",
    "                _loader = load_npz_frames\n",
    "                _transform = transform\n",
    "                _target_transform = target_transform\n",
    "\n",
    "            elif duration is not None:\n",
    "                assert duration > 0 and isinstance(duration, int)\n",
    "                frames_np_root = os.path.join(root, f'duration_{duration}')\n",
    "\n",
    "                if os.path.exists(frames_np_root):\n",
    "                    print(f'The directory {frames_np_root} already exists')\n",
    "                else:\n",
    "                    os.mkdir(frames_np_root)\n",
    "                    print(f'Mkdir [{frames_np_root}]')\n",
    "\n",
    "                    create_same_directory_structure(events_np_root, frames_np_root)\n",
    "\n",
    "                    t_ckp = time.time()\n",
    "                    for e_root, e_dirs, e_files in os.walk(events_np_root):\n",
    "                        if e_files.__len__() > 0:\n",
    "                            output_dir = os.path.join(frames_np_root, os.path.relpath(e_root, events_np_root))\n",
    "                            for e_file in e_files:\n",
    "                                events_np_file = os.path.join(e_root, e_file)\n",
    "                                print(f'Start to integrate [{events_np_file}] to frames and save to [{output_dir}].')\n",
    "                                # tpe.submit(integrate_events_file_to_frames_file_by_fixed_duration, self.load_events_np, events_np_file, output_dir, duration, H, W, True)\n",
    "                                integrate_events_file_to_frames_file_by_fixed_duration(self.load_events_np, events_np_file, output_dir, duration, H, W, True) \n",
    "\n",
    "                    print(f'Used time = [{round(time.time() - t_ckp, 2)}s].')\n",
    "\n",
    "                _root = frames_np_root\n",
    "                _loader = load_npz_frames\n",
    "                _transform = transform\n",
    "                _target_transform = target_transform\n",
    "\n",
    "            elif custom_integrate_function is not None:\n",
    "                if custom_integrated_frames_dir_name is None:\n",
    "                    custom_integrated_frames_dir_name = custom_integrate_function.__name__\n",
    "\n",
    "                frames_np_root = os.path.join(root, custom_integrated_frames_dir_name)\n",
    "                if os.path.exists(frames_np_root):\n",
    "                    print(f'The directory [{frames_np_root}] already exists.')\n",
    "                else:\n",
    "                    os.mkdir(frames_np_root)\n",
    "                    print(f'Mkdir [{frames_np_root}].')\n",
    "                    # create the same directory structure\n",
    "                    create_same_directory_structure(events_np_root, frames_np_root)\n",
    "                    # use multi-thread to accelerate\n",
    "                    t_ckp = time.time()\n",
    "                    for e_root, e_dirs, e_files in os.walk(events_np_root):\n",
    "                        if e_files.__len__() > 0:\n",
    "                            output_dir = os.path.join(frames_np_root, os.path.relpath(e_root, events_np_root))\n",
    "                            for e_file in e_files:\n",
    "                                events_np_file = os.path.join(e_root, e_file)\n",
    "                                print(f'Start to integrate [{events_np_file}] to frames and save to [{output_dir}].')\n",
    "                                # tpe.submit(save_frames_to_npz_and_print, os.path.join(output_dir, os.path.basename(events_np_file)), custom_integrate_function(np.load(events_np_file), H, W))\n",
    "                                save_frames_to_npz_and_print(os.path.join(output_dir, os.path.basename(events_np_file)), custom_integrate_function(np.load(events_np_file), H, W))\n",
    "\n",
    "                    print(f'Used time = [{round(time.time() - t_ckp, 2)}s].')\n",
    "\n",
    "                _root = frames_np_root\n",
    "                _loader = load_npz_frames\n",
    "                _transform = transform\n",
    "                _target_transform = target_transform\n",
    "\n",
    "\n",
    "            else:\n",
    "                raise ValueError('At least one of \"frames_number\", \"duration\" and \"custom_integrate_function\" should not be None.')\n",
    "\n",
    "        if train is not None:\n",
    "            if train:\n",
    "                _root = os.path.join(_root, 'train')\n",
    "            else:\n",
    "                _root = os.path.join(_root, 'test')\n",
    "\n",
    "        super().__init__(root=_root, loader=_loader, extensions=('.npz', ), transform=_transform,\n",
    "                         target_transform=_target_transform)\n",
    "    \n",
    "\n",
    "    @staticmethod\n",
    "    @abstractmethod\n",
    "    def create_events_np_files(extract_root: str, events_np_root: str):\n",
    "        '''\n",
    "        :param extract_root: Root directory path which saves extracted files from downloaded files\n",
    "        :type extract_root: str\n",
    "        :param events_np_root: Root directory path which saves events files in the ``npz`` format\n",
    "        :type events_np_root:\n",
    "        :return: None\n",
    "        This function defines how to convert the origin binary data in ``extract_root`` to ``npz`` format and save converted files in ``events_np_root``.\n",
    "        '''\n",
    "        pass\n",
    "\n",
    "    @staticmethod\n",
    "    @abstractmethod\n",
    "    def get_H_W() -> Tuple:\n",
    "        '''\n",
    "        :return: A tuple ``(H, W)``, where ``H`` is the height of the data and ``W`` is the weight of the data.\n",
    "            For example, this function returns ``(128, 128)`` for the DVS128 Gesture dataset.\n",
    "        :rtype: tuple\n",
    "        '''\n",
    "        pass\n",
    "\n",
    "    @staticmethod\n",
    "    def load_events_np(fname: str):\n",
    "        '''\n",
    "        :param fname: file name\n",
    "        :return: a dict whose keys are ``['t', 'x', 'y', 'p']`` and values are ``numpy.ndarray``\n",
    "        This function defines how to load a sample from `events_np`. In most cases, this function is `np.load`.\n",
    "        But for some datasets, e.g., ES-ImageNet, it can be different.\n",
    "        '''\n",
    "        return np.load(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4a744365",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FYPDataset(NeuromorphicDatasetFolder):\n",
    "    def __init__(\n",
    "            self,\n",
    "            root: str,\n",
    "            train: bool = None,\n",
    "            data_type: str = 'event',\n",
    "            frames_number: int = None,\n",
    "            split_by: str = None,\n",
    "            duration: int = None,\n",
    "            custom_integrate_function: Callable = None,\n",
    "            custom_integrated_frames_dir_name: str = None,\n",
    "            transform: Optional[Callable] = None,\n",
    "            target_transform: Optional[Callable] = None,\n",
    "    ) -> None:\n",
    "        assert train is not None\n",
    "        super().__init__(root, train, data_type, frames_number, split_by, duration, custom_integrate_function, custom_integrated_frames_dir_name, transform, target_transform)\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def load_origin_data(file_name: str) -> Dict:\n",
    "        '''\n",
    "        :param file_name: path of the raw data file\n",
    "        :type file_name: str\n",
    "        :return: a dict where the keys are ``['x', 'y', 'p', 't']`` and values are ``numpy.ndarray``\n",
    "        :rtype: Dict\n",
    "\n",
    "        This function is written by referring to https://github.com/fangwei123456/spikingjelly/blob/master/spikingjelly/datasets/__init__.py\n",
    "        '''\n",
    "        '''eventually make it so that the filename is sent, but for testing purposes, we only use this one file'''\n",
    "        df = pd.read_csv(file_name, names=[\"x\", \"y\", \"p\", \"t\"], sep=\",\")\n",
    "        \n",
    "        data = {\n",
    "            'x': np.array(df['x']),\n",
    "            'y': np.array(df['y']),\n",
    "            'p': np.array(df['p']),\n",
    "            't': np.array(df['t'])\n",
    "        }\n",
    "\n",
    "        return data\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def split_raw_to_np(fname: str, raw_file: str, csv_file: str, output_dir: str):\n",
    "        '''\n",
    "        :param fname: the filename of the file to split\n",
    "        :type fname: str\n",
    "        :param raw_file: the raw data that we want to split\n",
    "        :type raw_file: str\n",
    "        :param csv_file: the labels of the raw data file\n",
    "        :type csv_file: str\n",
    "        :param output_dir: the directory of the output files\n",
    "        :type output_dir: str\n",
    "\n",
    "        this function defines how to split a raw file into several smaller files which contain information \n",
    "        from one event as defined in our labelled csv_file\n",
    "        '''\n",
    "        global np_savez\n",
    "        events = FYPDataset.load_origin_data(raw_file)\n",
    "        print(f'Start to split [{raw_file}] to samples')\n",
    "\n",
    "        # read csv file and get time stamps and label of each sample\n",
    "        csv_data = np.loadtxt(csv_file, dtype=np.uint32, delimiter=',', skiprows=1)\n",
    "\n",
    "        label_file_num = {}\n",
    "        for i in range(csv_data.shape[0]):\n",
    "            print(f'{i}th iteration of np splitting for {raw_file}')\n",
    "            label = csv_data[i][0]\n",
    "            \n",
    "            # make directory for labels when we come across the label for the first time\n",
    "            if label not in label_file_num.keys():\n",
    "                label_file_num[label] = 0\n",
    "                if not os.path.isdir(os.path.join(output_dir, str(label))):\n",
    "                    os.mkdir(os.path.join(output_dir, str(label)))\n",
    "\n",
    "            t_start = csv_data[i][1]\n",
    "            t_end = csv_data[i][2]\n",
    "            mask = np.logical_and(events['t'] >= t_start, events['t'] < t_end)\n",
    "            file_name = os.path.join(output_dir, str(label), f'{fname}_{label_file_num[label]}.npz')\n",
    "            np_savez(file_name, x=events['x'][mask], y=events['y'][mask], p=events['p'][mask], t=events['t'][mask])\n",
    "            print(f'[{fname}] saved')\n",
    "            label_file_num[label] += 1\n",
    "\n",
    "    \n",
    "    @staticmethod\n",
    "    def create_events_np_files(extract_root: str, events_np_root: str):\n",
    "        '''\n",
    "        :param extract_root: the root directory of the dataset\n",
    "        :type extract_root: str\n",
    "        :param events_np_root: the root directory of the split files\n",
    "        :type events_np_root: str\n",
    "\n",
    "        this function is the driver function which will go through all files in the dataset and split them\n",
    "        into individual samples in the train and test sets\n",
    "        '''\n",
    "        raw_dir = os.path.join(extract_root)\n",
    "        train_dir = os.path.join(events_np_root, 'train')\n",
    "        test_dir = os.path.join(events_np_root, 'test')\n",
    "\n",
    "        if not os.path.isdir(train_dir):\n",
    "            os.mkdir(train_dir)\n",
    "            os.mkdir(test_dir)\n",
    "        \n",
    "        with open(os.path.join(raw_dir, 'trials_to_train.txt')) as trials_to_train_txt, open(\n",
    "            os.path.join(raw_dir, 'trials_to_test.txt')) as trials_to_test_txt:\n",
    "\n",
    "            # use multi-thread to accelerate the process\n",
    "            \n",
    "            t_ckp = time.time()\n",
    "\n",
    "            for fname in trials_to_train_txt.readlines():\n",
    "                fname = fname.strip()\n",
    "                print(fname, end=',')\n",
    "                if fname.__len__() > 0:\n",
    "                    raw_file = os.path.join(raw_dir, fname)\n",
    "                    fname = os.path.splitext(fname)[0]\n",
    "                    # tpe.submit(FYPDataset.split_raw_to_np, fname, raw_file, os.path.join(raw_dir, fname), train_dir)\n",
    "                    FYPDataset.split_raw_to_np(fname, raw_file, os.path.join(raw_dir, f'{fname}_labels.txt'), train_dir)\n",
    "\n",
    "            for fname in trials_to_test_txt.readlines():\n",
    "                fname = fname.strip()\n",
    "                print(fname, end=',')\n",
    "                if fname.__len__() > 0:\n",
    "                    raw_file = os.path.join(raw_dir, fname)\n",
    "                    fname = os.path.splitext(fname)[0]\n",
    "                    # tpe.submit(FYPDataset.split_raw_to_np, fname, raw_file, os.path.join(raw_dir, fname), test_dir)\n",
    "                    FYPDataset.split_raw_to_np(fname, raw_file, os.path.join(raw_dir, f'{fname}_labels.txt'), test_dir)\n",
    "                \n",
    "            print(f'Used time = [{round(time.time() - t_ckp, 2)}s].')\n",
    "        print(f'All raw files have been split to samples and saved into [{train_dir, test_dir}]')\n",
    "\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def get_H_W() -> Tuple:\n",
    "        '''\n",
    "        this function returns the height and width of the frame\n",
    "        '''\n",
    "        return 480, 640"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a349122b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "class VotingLayer(nn.Module):\n",
    "    def __init__(self, voter_num: int):\n",
    "        super().__init__()\n",
    "        self.voting = nn.AvgPool1d(voter_num, voter_num)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        return self.voting(x.unsqueeze(1)).squeeze(1)\n",
    "\n",
    "\n",
    "# In[7]:\n",
    "\n",
    "\n",
    "class PythonNet(nn.Module):\n",
    "    def __init__(self, channels: int):\n",
    "        super().__init__()\n",
    "        conv = []\n",
    "        conv.extend(PythonNet.conv3x3(2, channels))\n",
    "        conv.append(nn.MaxPool2d(2, 2))\n",
    "        \n",
    "        for i in range(4):\n",
    "            conv.extend(PythonNet.conv3x3(channels, channels))\n",
    "            conv.append(nn.MaxPool2d(2, 2))\n",
    "        \n",
    "        self.conv = nn.Sequential(*conv)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            layer.Dropout(0.5),\n",
    "            nn.Linear(channels * 4 * 4, channels * 2 * 2, bias=False),\n",
    "            neuron.LIFNode(tau=2.0, surrogate_function=surrogate.ATan(), detach_reset=True),\n",
    "            layer.Dropout(0.5),\n",
    "            nn.Linear(channels * 2 * 2, 20, bias=False),\n",
    "            neuron.LIFNode(tau=2.0, surrogate_function=surrogate.ATan(), detach_reset=True)\n",
    "        )\n",
    "        \n",
    "        self.vote = VotingLayer(10)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = x.permute(1, 0, 2, 3, 4)\n",
    "        torch.reshape(x, (128, 128))\n",
    "        out_spikes = self.vote(self.fc(self.conv(x[0])))\n",
    "        for t in range(1, x.shape[0]):\n",
    "            out_spikes += self.vote(self.fc(self.conv(x[t])))\n",
    "        return out_spikes / x.shape[0]\n",
    "    \n",
    "    @staticmethod\n",
    "    def conv3x3(in_channels: int, out_channels):\n",
    "        return [\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            neuron.LIFNode(tau=2.0, surrogate_function=surrogate.ATan(), detach_reset=True)\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ed7e0bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# argument parsing\n",
    "snnparser = argparse.ArgumentParser(description='Classify DVS128 Gesture')\n",
    "snnparser.add_argument('-T', default=16, type=int, help='simulating time-steps')\n",
    "snnparser.add_argument('-device', default='cuda:0', help='device')\n",
    "snnparser.add_argument('-b', default=4, type=int, help='batch size')\n",
    "snnparser.add_argument('-epochs', default=64, type=int, metavar='N', help='number of total epochs to run')\n",
    "snnparser.add_argument('-j', default=4, type=int, metavar='N', help='number of data loading workers (default: 4)')\n",
    "snnparser.add_argument('-channels', default=128, type=int, help='channels of Conv2d in SNN')\n",
    "snnparser.add_argument('-data_dir', type=str, default='./', help='root dir')\n",
    "snnparser.add_argument('-out_dir', type=str, default='./output', help='root dir for saving logs and checkpoint')\n",
    "\n",
    "snnparser.add_argument('-lr', default=0.001, type=float, help='learning rate')\n",
    "snnparser.add_argument('-momentum', default=0.9, type=float, help='momentum for SGD')\n",
    "snnparser.add_argument('-lr_scheduler', default='CosALR', type=str, help='use which schedule. StepLR or CosALR')\n",
    "snnparser.add_argument('-step_size', default=32, type=float, help='step_size for StepLR')\n",
    "snnparser.add_argument('-gamma', default=0.1, type=float, help='gamma for StepLR')\n",
    "snnparser.add_argument('-T_max', default=32, type=int, help='T_max for CosineAnnealingLR')\n",
    "\n",
    "args = snnparser.parse_args(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "536e670e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start to convert the origin data from [./data] to [./events_np] in np.ndarray format.\n",
      "test.csv,Start to split [./data\\test.csv] to samples\n",
      "0th iteration of np splitting for ./data\\test.csv\n",
      "[test] saved\n",
      "1th iteration of np splitting for ./data\\test.csv\n",
      "[test] saved\n",
      "2th iteration of np splitting for ./data\\test.csv\n",
      "[test] saved\n",
      "3th iteration of np splitting for ./data\\test.csv\n",
      "[test] saved\n",
      "4th iteration of np splitting for ./data\\test.csv\n",
      "[test] saved\n",
      "5th iteration of np splitting for ./data\\test.csv\n",
      "[test] saved\n",
      "6th iteration of np splitting for ./data\\test.csv\n",
      "[test] saved\n",
      "7th iteration of np splitting for ./data\\test.csv\n",
      "[test] saved\n",
      "8th iteration of np splitting for ./data\\test.csv\n",
      "[test] saved\n",
      "9th iteration of np splitting for ./data\\test.csv\n",
      "[test] saved\n",
      "10th iteration of np splitting for ./data\\test.csv\n",
      "[test] saved\n",
      "11th iteration of np splitting for ./data\\test.csv\n",
      "[test] saved\n",
      "12th iteration of np splitting for ./data\\test.csv\n",
      "[test] saved\n",
      "13th iteration of np splitting for ./data\\test.csv\n",
      "[test] saved\n",
      "14th iteration of np splitting for ./data\\test.csv\n",
      "[test] saved\n",
      "15th iteration of np splitting for ./data\\test.csv\n",
      "[test] saved\n",
      "16th iteration of np splitting for ./data\\test.csv\n",
      "[test] saved\n",
      "test.csv,Start to split [./data\\test.csv] to samples\n",
      "0th iteration of np splitting for ./data\\test.csv\n",
      "[test] saved\n",
      "1th iteration of np splitting for ./data\\test.csv\n",
      "[test] saved\n",
      "2th iteration of np splitting for ./data\\test.csv\n",
      "[test] saved\n",
      "3th iteration of np splitting for ./data\\test.csv\n",
      "[test] saved\n",
      "4th iteration of np splitting for ./data\\test.csv\n",
      "[test] saved\n",
      "5th iteration of np splitting for ./data\\test.csv\n",
      "[test] saved\n",
      "6th iteration of np splitting for ./data\\test.csv\n",
      "[test] saved\n",
      "7th iteration of np splitting for ./data\\test.csv\n",
      "[test] saved\n",
      "8th iteration of np splitting for ./data\\test.csv\n",
      "[test] saved\n",
      "9th iteration of np splitting for ./data\\test.csv\n",
      "[test] saved\n",
      "10th iteration of np splitting for ./data\\test.csv\n",
      "[test] saved\n",
      "11th iteration of np splitting for ./data\\test.csv\n",
      "[test] saved\n",
      "12th iteration of np splitting for ./data\\test.csv\n",
      "[test] saved\n",
      "13th iteration of np splitting for ./data\\test.csv\n",
      "[test] saved\n",
      "14th iteration of np splitting for ./data\\test.csv\n",
      "[test] saved\n",
      "15th iteration of np splitting for ./data\\test.csv\n",
      "[test] saved\n",
      "16th iteration of np splitting for ./data\\test.csv\n",
      "[test] saved\n",
      "Used time = [0.03s].\n",
      "All raw files have been split to samples and saved into [('./events_np\\\\train', './events_np\\\\test')]\n",
      "The directory ./frames_number_16_split_by_number already exists\n",
      "Start to convert the origin data from [./data] to [./events_np] in np.ndarray format.\n",
      "test.csv,Start to split [./data\\test.csv] to samples\n",
      "0th iteration of np splitting for ./data\\test.csv\n",
      "[test] saved\n",
      "1th iteration of np splitting for ./data\\test.csv\n",
      "[test] saved\n",
      "2th iteration of np splitting for ./data\\test.csv\n",
      "[test] saved\n",
      "3th iteration of np splitting for ./data\\test.csv\n",
      "[test] saved\n",
      "4th iteration of np splitting for ./data\\test.csv\n",
      "[test] saved\n",
      "5th iteration of np splitting for ./data\\test.csv\n",
      "[test] saved\n",
      "6th iteration of np splitting for ./data\\test.csv\n",
      "[test] saved\n",
      "7th iteration of np splitting for ./data\\test.csv\n",
      "[test] saved\n",
      "8th iteration of np splitting for ./data\\test.csv\n",
      "[test] saved\n",
      "9th iteration of np splitting for ./data\\test.csv\n",
      "[test] saved\n",
      "10th iteration of np splitting for ./data\\test.csv\n",
      "[test] saved\n",
      "11th iteration of np splitting for ./data\\test.csv\n",
      "[test] saved\n",
      "12th iteration of np splitting for ./data\\test.csv\n",
      "[test] saved\n",
      "13th iteration of np splitting for ./data\\test.csv\n",
      "[test] saved\n",
      "14th iteration of np splitting for ./data\\test.csv\n",
      "[test] saved\n",
      "15th iteration of np splitting for ./data\\test.csv\n",
      "[test] saved\n",
      "16th iteration of np splitting for ./data\\test.csv\n",
      "[test] saved\n",
      "test.csv,Start to split [./data\\test.csv] to samples\n",
      "0th iteration of np splitting for ./data\\test.csv\n",
      "[test] saved\n",
      "1th iteration of np splitting for ./data\\test.csv\n",
      "[test] saved\n",
      "2th iteration of np splitting for ./data\\test.csv\n",
      "[test] saved\n",
      "3th iteration of np splitting for ./data\\test.csv\n",
      "[test] saved\n",
      "4th iteration of np splitting for ./data\\test.csv\n",
      "[test] saved\n",
      "5th iteration of np splitting for ./data\\test.csv\n",
      "[test] saved\n",
      "6th iteration of np splitting for ./data\\test.csv\n",
      "[test] saved\n",
      "7th iteration of np splitting for ./data\\test.csv\n",
      "[test] saved\n",
      "8th iteration of np splitting for ./data\\test.csv\n",
      "[test] saved\n",
      "9th iteration of np splitting for ./data\\test.csv\n",
      "[test] saved\n",
      "10th iteration of np splitting for ./data\\test.csv\n",
      "[test] saved\n",
      "11th iteration of np splitting for ./data\\test.csv\n",
      "[test] saved\n",
      "12th iteration of np splitting for ./data\\test.csv\n",
      "[test] saved\n",
      "13th iteration of np splitting for ./data\\test.csv\n",
      "[test] saved\n",
      "14th iteration of np splitting for ./data\\test.csv\n",
      "[test] saved\n",
      "15th iteration of np splitting for ./data\\test.csv\n",
      "[test] saved\n",
      "16th iteration of np splitting for ./data\\test.csv\n",
      "[test] saved\n",
      "Used time = [0.04s].\n",
      "All raw files have been split to samples and saved into [('./events_np\\\\train', './events_np\\\\test')]\n",
      "The directory ./frames_number_16_split_by_number already exists\n"
     ]
    }
   ],
   "source": [
    "net = PythonNet(channels=args.channels)\n",
    "net.to(args.device)\n",
    "\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=args.lr, momentum=args.momentum)\n",
    "\n",
    "lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=args.T_max)\n",
    "\n",
    "train_set = FYPDataset(args.data_dir, train=True, data_type='frame', split_by='number', frames_number=args.T)\n",
    "test_set = FYPDataset(args.data_dir, train=False, data_type='frame', split_by='number', frames_number=args.T)\n",
    "\n",
    "train_data_loader = DataLoader(\n",
    "    dataset=train_set,\n",
    "    batch_size=args.b,\n",
    "    shuffle=True,\n",
    "    num_workers=args.j,\n",
    "    drop_last=True,\n",
    "    pin_memory=True)\n",
    "\n",
    "test_data_loader = DataLoader(\n",
    "    dataset=test_set,\n",
    "    batch_size=args.b,\n",
    "    shuffle=False,\n",
    "    num_workers=args.j,\n",
    "    drop_last=False,\n",
    "    pin_memory=True)\n",
    "\n",
    "scaler = amp.GradScaler()\n",
    "\n",
    "start_epoch = 0\n",
    "max_test_acc = 0\n",
    "\n",
    "out_dir = os.path.join(args.out_dir, f'T_{args.T}_b_{args.b}_c_{args.channels}_SGD_lr_{args.lr}_')\n",
    "out_dir += f'CosALR_{args.T_max}'\n",
    "out_dir += '_amp'\n",
    "\n",
    "\n",
    "if not os.path.exists(out_dir):\n",
    "    os.mkdir(out_dir)\n",
    "    print(f'Mkdir {out_dir}.')\n",
    "\n",
    "with open(os.path.join(out_dir, 'args.txt'), 'w', encoding='utf-8') as args_txt:\n",
    "    args_txt.write(str(args))\n",
    "\n",
    "writer = SummaryWriter(os.path.join(out_dir, 'dvsg_logs'), purge_step=start_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "90813f76",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "DataLoader worker (pid(s) 18392, 10300, 19080, 2896) exited unexpectedly",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mEmpty\u001b[0m                                     Traceback (most recent call last)",
      "File \u001b[1;32mE:\\uni-stuff\\FYP\\gesture-recognition-snn\\snnenv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:990\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    <a href='file:///e%3A/uni-stuff/FYP/gesture-recognition-snn/snnenv/lib/site-packages/torch/utils/data/dataloader.py?line=988'>989</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> <a href='file:///e%3A/uni-stuff/FYP/gesture-recognition-snn/snnenv/lib/site-packages/torch/utils/data/dataloader.py?line=989'>990</a>\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_data_queue\u001b[39m.\u001b[39;49mget(timeout\u001b[39m=\u001b[39;49mtimeout)\n\u001b[0;32m    <a href='file:///e%3A/uni-stuff/FYP/gesture-recognition-snn/snnenv/lib/site-packages/torch/utils/data/dataloader.py?line=990'>991</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m (\u001b[39mTrue\u001b[39;00m, data)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\queue.py:178\u001b[0m, in \u001b[0;36mQueue.get\u001b[1;34m(self, block, timeout)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/Jatin/AppData/Local/Programs/Python/Python38/lib/queue.py?line=176'>177</a>\u001b[0m \u001b[39mif\u001b[39;00m remaining \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m:\n\u001b[1;32m--> <a href='file:///c%3A/Users/Jatin/AppData/Local/Programs/Python/Python38/lib/queue.py?line=177'>178</a>\u001b[0m     \u001b[39mraise\u001b[39;00m Empty\n\u001b[0;32m    <a href='file:///c%3A/Users/Jatin/AppData/Local/Programs/Python/Python38/lib/queue.py?line=178'>179</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnot_empty\u001b[39m.\u001b[39mwait(remaining)\n",
      "\u001b[1;31mEmpty\u001b[0m: ",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32me:\\uni-stuff\\FYP\\gesture-recognition-snn\\gesture-model.ipynb Cell 9'\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/uni-stuff/FYP/gesture-recognition-snn/gesture-model.ipynb#ch0000008?line=4'>5</a>\u001b[0m train_acc \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/uni-stuff/FYP/gesture-recognition-snn/gesture-model.ipynb#ch0000008?line=5'>6</a>\u001b[0m train_samples \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/uni-stuff/FYP/gesture-recognition-snn/gesture-model.ipynb#ch0000008?line=6'>7</a>\u001b[0m \u001b[39mfor\u001b[39;00m frame, label \u001b[39min\u001b[39;00m train_data_loader:\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/uni-stuff/FYP/gesture-recognition-snn/gesture-model.ipynb#ch0000008?line=7'>8</a>\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/uni-stuff/FYP/gesture-recognition-snn/gesture-model.ipynb#ch0000008?line=8'>9</a>\u001b[0m     frame \u001b[39m=\u001b[39m frame\u001b[39m.\u001b[39mfloat()\u001b[39m.\u001b[39mto(args\u001b[39m.\u001b[39mdevice)\n",
      "File \u001b[1;32mE:\\uni-stuff\\FYP\\gesture-recognition-snn\\snnenv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:521\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    <a href='file:///e%3A/uni-stuff/FYP/gesture-recognition-snn/snnenv/lib/site-packages/torch/utils/data/dataloader.py?line=518'>519</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    <a href='file:///e%3A/uni-stuff/FYP/gesture-recognition-snn/snnenv/lib/site-packages/torch/utils/data/dataloader.py?line=519'>520</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()\n\u001b[1;32m--> <a href='file:///e%3A/uni-stuff/FYP/gesture-recognition-snn/snnenv/lib/site-packages/torch/utils/data/dataloader.py?line=520'>521</a>\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    <a href='file:///e%3A/uni-stuff/FYP/gesture-recognition-snn/snnenv/lib/site-packages/torch/utils/data/dataloader.py?line=521'>522</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    <a href='file:///e%3A/uni-stuff/FYP/gesture-recognition-snn/snnenv/lib/site-packages/torch/utils/data/dataloader.py?line=522'>523</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    <a href='file:///e%3A/uni-stuff/FYP/gesture-recognition-snn/snnenv/lib/site-packages/torch/utils/data/dataloader.py?line=523'>524</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    <a href='file:///e%3A/uni-stuff/FYP/gesture-recognition-snn/snnenv/lib/site-packages/torch/utils/data/dataloader.py?line=524'>525</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mE:\\uni-stuff\\FYP\\gesture-recognition-snn\\snnenv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1186\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   <a href='file:///e%3A/uni-stuff/FYP/gesture-recognition-snn/snnenv/lib/site-packages/torch/utils/data/dataloader.py?line=1182'>1183</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_process_data(data)\n\u001b[0;32m   <a href='file:///e%3A/uni-stuff/FYP/gesture-recognition-snn/snnenv/lib/site-packages/torch/utils/data/dataloader.py?line=1184'>1185</a>\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_shutdown \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tasks_outstanding \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m-> <a href='file:///e%3A/uni-stuff/FYP/gesture-recognition-snn/snnenv/lib/site-packages/torch/utils/data/dataloader.py?line=1185'>1186</a>\u001b[0m idx, data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_data()\n\u001b[0;32m   <a href='file:///e%3A/uni-stuff/FYP/gesture-recognition-snn/snnenv/lib/site-packages/torch/utils/data/dataloader.py?line=1186'>1187</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tasks_outstanding \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m   <a href='file:///e%3A/uni-stuff/FYP/gesture-recognition-snn/snnenv/lib/site-packages/torch/utils/data/dataloader.py?line=1187'>1188</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable:\n\u001b[0;32m   <a href='file:///e%3A/uni-stuff/FYP/gesture-recognition-snn/snnenv/lib/site-packages/torch/utils/data/dataloader.py?line=1188'>1189</a>\u001b[0m     \u001b[39m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[1;32mE:\\uni-stuff\\FYP\\gesture-recognition-snn\\snnenv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1142\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   <a href='file:///e%3A/uni-stuff/FYP/gesture-recognition-snn/snnenv/lib/site-packages/torch/utils/data/dataloader.py?line=1139'>1140</a>\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m   <a href='file:///e%3A/uni-stuff/FYP/gesture-recognition-snn/snnenv/lib/site-packages/torch/utils/data/dataloader.py?line=1140'>1141</a>\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_thread\u001b[39m.\u001b[39mis_alive():\n\u001b[1;32m-> <a href='file:///e%3A/uni-stuff/FYP/gesture-recognition-snn/snnenv/lib/site-packages/torch/utils/data/dataloader.py?line=1141'>1142</a>\u001b[0m         success, data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_try_get_data()\n\u001b[0;32m   <a href='file:///e%3A/uni-stuff/FYP/gesture-recognition-snn/snnenv/lib/site-packages/torch/utils/data/dataloader.py?line=1142'>1143</a>\u001b[0m         \u001b[39mif\u001b[39;00m success:\n\u001b[0;32m   <a href='file:///e%3A/uni-stuff/FYP/gesture-recognition-snn/snnenv/lib/site-packages/torch/utils/data/dataloader.py?line=1143'>1144</a>\u001b[0m             \u001b[39mreturn\u001b[39;00m data\n",
      "File \u001b[1;32mE:\\uni-stuff\\FYP\\gesture-recognition-snn\\snnenv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1003\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m   <a href='file:///e%3A/uni-stuff/FYP/gesture-recognition-snn/snnenv/lib/site-packages/torch/utils/data/dataloader.py?line=1000'>1001</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(failed_workers) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m   <a href='file:///e%3A/uni-stuff/FYP/gesture-recognition-snn/snnenv/lib/site-packages/torch/utils/data/dataloader.py?line=1001'>1002</a>\u001b[0m     pids_str \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(\u001b[39mstr\u001b[39m(w\u001b[39m.\u001b[39mpid) \u001b[39mfor\u001b[39;00m w \u001b[39min\u001b[39;00m failed_workers)\n\u001b[1;32m-> <a href='file:///e%3A/uni-stuff/FYP/gesture-recognition-snn/snnenv/lib/site-packages/torch/utils/data/dataloader.py?line=1002'>1003</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mDataLoader worker (pid(s) \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m) exited unexpectedly\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(pids_str)) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[0;32m   <a href='file:///e%3A/uni-stuff/FYP/gesture-recognition-snn/snnenv/lib/site-packages/torch/utils/data/dataloader.py?line=1003'>1004</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(e, queue\u001b[39m.\u001b[39mEmpty):\n\u001b[0;32m   <a href='file:///e%3A/uni-stuff/FYP/gesture-recognition-snn/snnenv/lib/site-packages/torch/utils/data/dataloader.py?line=1004'>1005</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m (\u001b[39mFalse\u001b[39;00m, \u001b[39mNone\u001b[39;00m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: DataLoader worker (pid(s) 18392, 10300, 19080, 2896) exited unexpectedly"
     ]
    }
   ],
   "source": [
    "for epoch in range(start_epoch, args.epochs):\n",
    "    start_time = time.time()\n",
    "    net.train()\n",
    "    train_loss = 0\n",
    "    train_acc = 0\n",
    "    train_samples = 0\n",
    "    for frame, label in train_data_loader:\n",
    "        optimizer.zero_grad()\n",
    "        frame = frame.float().to(args.device)\n",
    "        label = label.to(args.device)\n",
    "        label_onehot = F.one_hot(label, 11).float()\n",
    "        # if args.amp:\n",
    "        with amp.autocast():\n",
    "            out_fr = net(frame)\n",
    "            loss = F.mse_loss(out_fr, label_onehot)\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        # else:\n",
    "        #     out_fr = net(frame)\n",
    "        #     loss = F.mse_loss(out_fr, label_onehot)\n",
    "        #     loss.backward()\n",
    "        #     optimizer.step()\n",
    "\n",
    "        train_samples += label.numel()\n",
    "        train_loss += loss.item() * label.numel()\n",
    "        train_acc += (out_fr.argmax(1) == label).float().sum().item()\n",
    "\n",
    "        functional.reset_net(net)\n",
    "    train_loss /= train_samples\n",
    "    train_acc /= train_samples\n",
    "\n",
    "    writer.add_scalar('train_loss', train_loss, epoch)\n",
    "    writer.add_scalar('train_acc', train_acc, epoch)\n",
    "    lr_scheduler.step()\n",
    "\n",
    "    net.eval()\n",
    "    test_loss = 0\n",
    "    test_acc = 0\n",
    "    test_samples = 0\n",
    "    with torch.no_grad():\n",
    "        for frame, label in test_data_loader:\n",
    "            frame = frame.float().to(args.device)\n",
    "            label = label.to(args.device)\n",
    "            label_onehot = F.one_hot(label, 11).float()\n",
    "            out_fr = net(frame)\n",
    "            loss = F.mse_loss(out_fr, label_onehot)\n",
    "\n",
    "            test_samples += label.numel()\n",
    "            test_loss += loss.item() * label.numel()\n",
    "            test_acc += (out_fr.argmax(1) == label).float().sum().item()\n",
    "            functional.reset_net(net)\n",
    "\n",
    "    test_loss /= test_samples\n",
    "    test_acc /= test_samples\n",
    "    writer.add_scalar('test_loss', test_loss, epoch)\n",
    "    writer.add_scalar('test_acc', test_acc, epoch)\n",
    "\n",
    "    save_max = False\n",
    "    if test_acc > max_test_acc:\n",
    "        max_test_acc = test_acc\n",
    "        save_max = True\n",
    "\n",
    "    checkpoint = {\n",
    "        'net': net.state_dict(),\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "        'lr_scheduler': lr_scheduler.state_dict(),\n",
    "        'epoch': epoch,\n",
    "        'max_test_acc': max_test_acc\n",
    "    }\n",
    "\n",
    "    if save_max:\n",
    "        torch.save(checkpoint, os.path.join(out_dir, 'checkpoint_max.pth'))\n",
    "\n",
    "    torch.save(checkpoint, os.path.join(out_dir, 'checkpoint_latest.pth'))\n",
    "\n",
    "    print(args)\n",
    "    print(f'epoch={epoch}, train_loss={train_loss}, train_acc={train_acc}, test_loss={test_loss}, test_acc={test_acc}, max_test_acc={max_test_acc}, total_time={time.time() - start_time}')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6edd0242087553ac2b51ddafede1f9afe2b0d82dcb62ae368f8598c88c368730"
  },
  "kernelspec": {
   "display_name": "snnenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
