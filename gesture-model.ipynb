{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8943aaf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "# python core imports\n",
    "from venv import create\n",
    "from typing import Callable, Dict, Optional, Tuple\n",
    "from abc import abstractmethod\n",
    "import struct\n",
    "import os\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import time\n",
    "import math\n",
    "from re import M\n",
    "import shutil\n",
    "import argparse\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "# python external modules\n",
    "import scipy.io\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from posixpath import split\n",
    "\n",
    "# torch imports\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.cuda import amp\n",
    "from torchvision.datasets import DatasetFolder, utils\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# spikingjelly imports\n",
    "from spikingjelly.clock_driven import functional, surrogate, layer\n",
    "import spikingjelly.event_driven.neuron as neuron\n",
    "import spikingjelly.event_driven.encoding as encoding\n",
    "from datasets.__init__ import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74221b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config variables\n",
    "np_savez = np.savez_compressed\n",
    "_seed_ = 2020\n",
    "torch.manual_seed(_seed_)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(_seed_)\n",
    "\n",
    "# logging setup for production only\n",
    "# logging.basicConfig(filename=f'logs/runlog.log', level=logging.DEBUG, format='%(asctime)s:%(levelname)s:%(message)s')\n",
    "# logger = logging.getLogger()\n",
    "# sys.stderr.write = logger.error\n",
    "# sys.stdout.write = logger.info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a349122b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "class VotingLayer(nn.Module):\n",
    "    def __init__(self, voter_num: int):\n",
    "        super().__init__()\n",
    "        self.voting = nn.AvgPool1d(voter_num, voter_num)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        return self.voting(x.unsqueeze(1)).squeeze(1)\n",
    "\n",
    "\n",
    "# In[7]:\n",
    "\n",
    "\n",
    "class PythonNet(nn.Module):\n",
    "    def __init__(self, channels: int):\n",
    "        super().__init__()\n",
    "        conv = []\n",
    "        conv.extend(PythonNet.conv3x3(2, channels))\n",
    "        conv.append(nn.MaxPool2d(2, 2))\n",
    "        \n",
    "        for i in range(4):\n",
    "            conv.extend(PythonNet.conv3x3(channels, channels))\n",
    "            conv.append(nn.MaxPool2d(2, 2))\n",
    "        \n",
    "        self.conv = nn.Sequential(*conv)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            layer.Dropout(0.5),\n",
    "            nn.Linear(channels * 4 * 4, channels * 2 * 2, bias=False),\n",
    "            neuron.LIFNode(tau=2.0, surrogate_function=surrogate.ATan(), detach_reset=True),\n",
    "            layer.Dropout(0.5),\n",
    "            nn.Linear(channels * 2 * 2, 20, bias=False),\n",
    "            neuron.LIFNode(tau=2.0, surrogate_function=surrogate.ATan(), detach_reset=True)\n",
    "        )\n",
    "        \n",
    "        self.vote = VotingLayer(10)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = x.permute(1, 0, 2, 3, 4)\n",
    "        print(x.shape)\n",
    "        torch.reshape(x, (x.shape[0], x.shape[1], x.shape[2], 128, 128))\n",
    "        out_spikes = self.vote(self.fc(self.conv(x[0])))\n",
    "        for t in range(1, x.shape[0]):\n",
    "            out_spikes += self.vote(self.fc(self.conv(x[t])))\n",
    "        return out_spikes / x.shape[0]\n",
    "    \n",
    "    @staticmethod\n",
    "    def conv3x3(in_channels: int, out_channels):\n",
    "        return [\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            neuron.LIFNode(tau=2.0, surrogate_function=surrogate.ATan(), detach_reset=True)\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7e0bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# argument parsing\n",
    "snnparser = argparse.ArgumentParser(description='Classify DVS128 Gesture')\n",
    "snnparser.add_argument('-T', default=16, type=int, help='simulating time-steps')\n",
    "snnparser.add_argument('-device', default='cuda:0', help='device')\n",
    "snnparser.add_argument('-b', default=1, type=int, help='batch size')\n",
    "snnparser.add_argument('-epochs', default=64, type=int, metavar='N', help='number of total epochs to run')\n",
    "snnparser.add_argument('-j', default=4, type=int, metavar='N', help='number of data loading workers (default: 4)')\n",
    "snnparser.add_argument('-channels', default=128, type=int, help='channels of Conv2d in SNN')\n",
    "snnparser.add_argument('-data_dir', type=str, default='./', help='root dir')\n",
    "snnparser.add_argument('-out_dir', type=str, default='./output', help='root dir for saving logs and checkpoint')\n",
    "\n",
    "snnparser.add_argument('-lr', default=0.001, type=float, help='learning rate')\n",
    "snnparser.add_argument('-momentum', default=0.9, type=float, help='momentum for SGD')\n",
    "snnparser.add_argument('-lr_scheduler', default='CosALR', type=str, help='use which schedule. StepLR or CosALR')\n",
    "snnparser.add_argument('-step_size', default=32, type=float, help='step_size for StepLR')\n",
    "snnparser.add_argument('-gamma', default=0.1, type=float, help='gamma for StepLR')\n",
    "snnparser.add_argument('-T_max', default=32, type=int, help='T_max for CosineAnnealingLR')\n",
    "\n",
    "args = snnparser.parse_args(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536e670e",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = PythonNet(channels=args.channels)\n",
    "net.to(args.device)\n",
    "\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=args.lr, momentum=args.momentum)\n",
    "\n",
    "lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=args.T_max)\n",
    "\n",
    "train_set = FYPDataset(args.data_dir, train=True, data_type='frame', split_by='number', frames_number=args.T)\n",
    "test_set = FYPDataset(args.data_dir, train=False, data_type='frame', split_by='number', frames_number=args.T)\n",
    "\n",
    "train_data_loader = DataLoader(\n",
    "    dataset=train_set,\n",
    "    batch_size=args.b,\n",
    "    shuffle=True,\n",
    "    num_workers=args.j,\n",
    "    drop_last=True,\n",
    "    pin_memory=True)\n",
    "\n",
    "test_data_loader = DataLoader(\n",
    "    dataset=test_set,\n",
    "    batch_size=args.b,\n",
    "    shuffle=False,\n",
    "    num_workers=args.j,\n",
    "    drop_last=False,\n",
    "    pin_memory=True)\n",
    "\n",
    "scaler = amp.GradScaler()\n",
    "\n",
    "start_epoch = 0\n",
    "max_test_acc = 0\n",
    "\n",
    "out_dir = os.path.join(args.out_dir, f'T_{args.T}_b_{args.b}_c_{args.channels}_SGD_lr_{args.lr}_')\n",
    "out_dir += f'CosALR_{args.T_max}'\n",
    "out_dir += '_amp'\n",
    "\n",
    "\n",
    "if not os.path.exists(out_dir):\n",
    "    os.mkdir(out_dir)\n",
    "    print(f'Mkdir {out_dir}.')\n",
    "\n",
    "with open(os.path.join(out_dir, 'args.txt'), 'w', encoding='utf-8') as args_txt:\n",
    "    args_txt.write(str(args))\n",
    "\n",
    "writer = SummaryWriter(os.path.join(out_dir, 'dvsg_logs'), purge_step=start_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90813f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    for epoch in range(start_epoch, args.epochs):\n",
    "        start_time = time.time()\n",
    "        net.train()\n",
    "        train_loss = 0\n",
    "        train_acc = 0\n",
    "        train_samples = 0\n",
    "        for frame, label in train_data_loader:\n",
    "            optimizer.zero_grad()\n",
    "            frame = frame.float().to(args.device)\n",
    "            label = label.to(args.device)\n",
    "            label_onehot = F.one_hot(label, 11).float()\n",
    "            # if args.amp:\n",
    "            with amp.autocast():\n",
    "                out_fr = net(frame)\n",
    "                loss = F.mse_loss(out_fr, label_onehot)\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            # else:\n",
    "            #     out_fr = net(frame)\n",
    "            #     loss = F.mse_loss(out_fr, label_onehot)\n",
    "            #     loss.backward()\n",
    "            #     optimizer.step()\n",
    "\n",
    "            train_samples += label.numel()\n",
    "            train_loss += loss.item() * label.numel()\n",
    "            train_acc += (out_fr.argmax(1) == label).float().sum().item()\n",
    "\n",
    "            functional.reset_net(net)\n",
    "        train_loss /= train_samples\n",
    "        train_acc /= train_samples\n",
    "\n",
    "        writer.add_scalar('train_loss', train_loss, epoch)\n",
    "        writer.add_scalar('train_acc', train_acc, epoch)\n",
    "        lr_scheduler.step()\n",
    "\n",
    "        net.eval()\n",
    "        test_loss = 0\n",
    "        test_acc = 0\n",
    "        test_samples = 0\n",
    "        with torch.no_grad():\n",
    "            for frame, label in test_data_loader:\n",
    "                frame = frame.float().to(args.device)\n",
    "                label = label.to(args.device)\n",
    "                label_onehot = F.one_hot(label, 11).float()\n",
    "                out_fr = net(frame)\n",
    "                loss = F.mse_loss(out_fr, label_onehot)\n",
    "\n",
    "                test_samples += label.numel()\n",
    "                test_loss += loss.item() * label.numel()\n",
    "                test_acc += (out_fr.argmax(1) == label).float().sum().item()\n",
    "                functional.reset_net(net)\n",
    "\n",
    "        test_loss /= test_samples\n",
    "        test_acc /= test_samples\n",
    "        writer.add_scalar('test_loss', test_loss, epoch)\n",
    "        writer.add_scalar('test_acc', test_acc, epoch)\n",
    "\n",
    "        save_max = False\n",
    "        if test_acc > max_test_acc:\n",
    "            max_test_acc = test_acc\n",
    "            save_max = True\n",
    "\n",
    "        checkpoint = {\n",
    "            'net': net.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'lr_scheduler': lr_scheduler.state_dict(),\n",
    "            'epoch': epoch,\n",
    "            'max_test_acc': max_test_acc\n",
    "        }\n",
    "\n",
    "        if save_max:\n",
    "            torch.save(checkpoint, os.path.join(out_dir, 'checkpoint_max.pth'))\n",
    "\n",
    "        torch.save(checkpoint, os.path.join(out_dir, 'checkpoint_latest.pth'))\n",
    "\n",
    "        print(args)\n",
    "        print(f'epoch={epoch}, train_loss={train_loss}, train_acc={train_acc}, test_loss={test_loss}, test_acc={test_acc}, max_test_acc={max_test_acc}, total_time={time.time() - start_time}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be05324",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trying using event parameter\n",
    "# neural network parameters\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, m, T):\n",
    "        super().__init__()\n",
    "        self.tempotron = neuron.Tempotron(480*640*m, 10, T)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        return self.tempotron(x, 'v_max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3b9509",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='spikingjelly Tempotron MNIST Training')\n",
    "\n",
    "parser.add_argument('--device', default='cuda:0', help='运行的设备，例如“cpu”或“cuda:0”\\n Device, e.g., \"cpu\" or \"cuda:0\"')\n",
    "\n",
    "parser.add_argument('--dataset-dir', default='./', help='保存MNIST数据集的位置，例如“./”\\n Root directory for saving MNIST dataset, e.g., \"./\"')\n",
    "parser.add_argument('--log-dir', default='./logs', help='保存tensorboard日志文件的位置，例如“./”\\n Root directory for saving tensorboard logs, e.g., \"./\"')\n",
    "parser.add_argument('--model-output-dir', default='./', help='模型保存路径，例如“./”\\n Model directory for saving, e.g., \"./\"')\n",
    "\n",
    "parser.add_argument('-b', '--batch-size', default=64, type=int, help='Batch 大小，例如“64”\\n Batch size, e.g., \"64\"')\n",
    "parser.add_argument('-T', '--timesteps', default=100, type=int, dest='T', help='仿真时长，例如“100”\\n Simulating timesteps, e.g., \"100\"')\n",
    "parser.add_argument('--lr', '--learning-rate', default=1e-3, type=float, metavar='LR', help='学习率，例如“1e-3”\\n Learning rate, e.g., \"1e-3\": ', dest='lr')\n",
    "# parser.add_argument('--tau', default=2.0, type=float, help='LIF神经元的时间常数tau，例如“100.0”\\n Membrane time constant, tau, for LIF neurons, e.g., \"100.0\"')\n",
    "parser.add_argument('-N', '--epoch', default=100, type=int, help='训练epoch，例如“100”\\n Training epoch, e.g., \"100\"')\n",
    "parser.add_argument('-m', default=16, type=int, help='使用高斯调谐曲线编码每个像素点使用的神经元数量，例如“16”\\n input neuron number for encoding a piexl in GaussianTuning encoder, e.g., \"16\"')\n",
    "\n",
    "args = parser.parse_args(\"\")\n",
    "device = args.device\n",
    "\n",
    "data_dir = args.dataset_dir\n",
    "log_dir = args.log_dir\n",
    "model_output_dir = args.model_output_dir\n",
    "\n",
    "batch_size = args.batch_size\n",
    "T = args.T\n",
    "learning_rate = args.lr\n",
    "train_epoch = args.epoch\n",
    "m = args.m\n",
    "\n",
    "encoder = encoding.GaussianTuning(n=1, m=m, x_min=torch.zeros(size=[1]).to(device), x_max=torch.ones(size=[1]).to(device))\n",
    "\n",
    "writer = SummaryWriter(log_dir)\n",
    "\n",
    "train_set = FYPDataset(data_dir, train=True, data_type='frame', split_by='number', frames_number=T)\n",
    "test_set = FYPDataset(data_dir, train=False, data_type='frame', split_by='number', frames_number=T)\n",
    "\n",
    "train_data_loader = DataLoader(\n",
    "    dataset=train_set,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "test_data_loader = DataLoader(\n",
    "    dataset= train_set,\n",
    "    batch_size = batch_size,\n",
    "    shuffle=False,\n",
    "    drop_last=False\n",
    ")\n",
    "\n",
    "net = Net(m, T).to(device)\n",
    "\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=learning_rate)\n",
    "\n",
    "train_times = 0\n",
    "max_test_accuracy = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753d7e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    for epoch in range(train_epoch):\n",
    "        print(\"Epoch {}:\".format(epoch))\n",
    "        print(\"Training...\")\n",
    "        net.train()\n",
    "        train_correct_sum = 0\n",
    "        train_sum = 0\n",
    "        for img, label in train_data_loader:\n",
    "            img = img.view(img.shape[0], -1).unsqueeze(1)  # [batch_size, 1, 784]\n",
    "            in_spikes = encoder.encode(img.to(device), T)  # [batch_size, 1, 784, m]\n",
    "            in_spikes = in_spikes.view(in_spikes.shape[0], -1)  # [batch_size, 784*m]\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            v_max = net(in_spikes)\n",
    "            loss = neuron.Tempotron.mse_loss(v_max, net.tempotron.v_threshold, label.to(device), 10)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_correct_sum += (v_max.argmax(dim=1) == label.to(device)).float().sum().item()\n",
    "            train_sum += label.numel()\n",
    "\n",
    "            train_batch_acc = (v_max.argmax(dim=1) == label.to(device)).float().mean().item()\n",
    "            writer.add_scalar('train_batch_acc', train_batch_acc, train_times)\n",
    "\n",
    "            train_times += 1\n",
    "        # train_accuracy = train_correct_sum / train_sum\n",
    "\n",
    "        print(\"Testing...\")\n",
    "        net.eval()\n",
    "        with torch.no_grad():\n",
    "            correct_num = 0\n",
    "            img_num = 0\n",
    "            for img, label in test_data_loader:\n",
    "                img = img.view(img.shape[0], -1).unsqueeze(1)  # [batch_size, 1, 784]\n",
    "\n",
    "                in_spikes = encoder.encode(img.to(device), T)  # [batch_size, 1, 784, m]\n",
    "                in_spikes = in_spikes.view(in_spikes.shape[0], -1)  # [batch_size, 784*m]\n",
    "                v_max = net(in_spikes)\n",
    "                correct_num += (v_max.argmax(dim=1) == label.to(device)).float().sum().item()\n",
    "                img_num += img.shape[0]\n",
    "            test_accuracy = correct_num / img_num\n",
    "            writer.add_scalar('test_accuracy', test_accuracy, epoch)\n",
    "            max_test_accuracy = max(max_test_accuracy, test_accuracy)\n",
    "        print(\"Epoch {}: train_acc = {}, test_acc={}, max_test_acc={}, train_times={}\".format(epoch, train_accuracy, test_accuracy, max_test_accuracy, train_times))\n",
    "        print()\n",
    "    \n",
    "    # 保存模型\n",
    "    torch.save(net, model_output_dir + \"/eventdata.ckpt\")\n",
    "    # 读取模型\n",
    "    # net = torch.load(model_output_dir + \"/tempotron_snn_mnist.ckpt\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6edd0242087553ac2b51ddafede1f9afe2b0d82dcb62ae368f8598c88c368730"
  },
  "kernelspec": {
   "display_name": "snnenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
