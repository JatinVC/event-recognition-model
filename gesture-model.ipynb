{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8943aaf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.cuda import amp\n",
    "from spikingjelly.clock_driven import functional, surrogate, layer, neuron\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from spikingjelly.datasets.dvs128_gesture import DVS128Gesture\n",
    "from datasets import eventdataset\n",
    "import time\n",
    "import os\n",
    "import argparse\n",
    "import numpy as np\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bca2425f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# configs\n",
    "_seed_ = 2020\n",
    "torch.manual_seed(_seed_)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(_seed_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "258f8507",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VotingLayer(nn.Module):\n",
    "    def __init__(self, voter_num: int):\n",
    "        super().__init__()\n",
    "        self.voting = nn.AvgPool1d(voter_num, voter_num)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        return self.voting(x.unsqueeze(1)).squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eaa29f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PythonNet(nn.Module):\n",
    "    def __init__(self, channels: int):\n",
    "        super().__init__()\n",
    "        conv = []\n",
    "        conv.extend(PythonNet.conv3x3(2, channels))\n",
    "        conv.append(nn.MaxPool2d(2, 2))\n",
    "        \n",
    "        for i in range(4):\n",
    "            conv.extend(PythonNet.conv3x3(channels, channels))\n",
    "            conv.append(nn.MaxPool2d(2, 2))\n",
    "        \n",
    "        self.conv = nn.Sequential(*conv)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            layer.Dropout(0.5),\n",
    "            nn.Linear(channels * 4 * 4, channels * 2 * 2, bias=False),\n",
    "            neuron.LIFNode(tau=2.0, surrogate_function=surrogate.ATan(), detach_reset=True),\n",
    "            layer.Dropout(0.5),\n",
    "            nn.Linear(channels * 2 * 2, 110, bias=False),\n",
    "            neuron.LIFNode(tau=2.0, surrogate_function=surrogate.ATan(), detach_reset=True)\n",
    "        )\n",
    "        \n",
    "        self.vote = VotingLayer(10)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = x.permute(1, 0, 2, 3, 4)\n",
    "        out_spikes = self.vote(self.fc(self.conv(x[0])))\n",
    "        for t in range(1, x.shape[0]):\n",
    "            out_spikes += self.vote(self.fc(self.conv(x[t])))\n",
    "        return out_spikes / x.shape[0]\n",
    "    \n",
    "    @staticmethod\n",
    "    def conv3x3(in_channels: int, out_channels):\n",
    "        return [\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            neuron.LIFNode(tau=2.0, surrogate_function=surrogate.ATan(), detach_reset=True)\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b689cf5e",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (137910983.py, line 98)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Input \u001b[1;32mIn [2]\u001b[1;36m\u001b[0m\n\u001b[1;33m    label_onehot = F.one_hot(lE:\\uni-stuff\\FYP\\gesture-recognition-snn\\datasets\\eventdataset.pyabel, 11).float()\u001b[0m\n\u001b[1;37m                               ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# def main():\n",
    "#     '''\n",
    "#     .. code:: bash\n",
    "#         usage: gesture-model.py [-h] [-T T] [-device DEVICE] [-b B] [-epochs N] [-j N] [-channels CHANNELS] [-data_dir DATA_DIR] [-out_dir OUT_DIR] [-resume RESUME] [-amp] [-cupy] [-opt OPT] [-lr LR] [-momentum MOMENTUM] [-lr_scheduler LR_SCHEDULER] [-step_size STEP_SIZE] [-gamma GAMMA] [-T_max T_MAX]\n",
    "#         Classify DVS128 Gesture\n",
    "#         optional arguments:\n",
    "#           -data_dir DATA_DIR    root dir of DVS128 Gesture dataset\n",
    "#           -out_dir OUT_DIR      root dir for saving logs and checkpoint\n",
    "#     '''\n",
    "#     parser = argparse.ArgumentParser(description='Classify DVS128 Gesture')\n",
    "#     parser.add_argument('-data_dir', type=str, help='root dir of DVS128 Gesture dataset')\n",
    "#     parser.add_argument('-out_dir', type=str, help='root dir for saving logs and checkpoint')\n",
    "\n",
    "#     args = parser.parse_args(['-data_dir', './dataset', '-out_dir', './logs'])\n",
    "#     print(args)\n",
    "    \n",
    "#     net = PythonNet(channels=128)\n",
    "#     print(net)\n",
    "#     net.to('cuda:0')\n",
    "\n",
    "#     optimizer = torch.optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "    \n",
    "#     lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=32)\n",
    "\n",
    "    \n",
    "#     train_set = DVS128Gesture(args.data_dir, train=True, data_type='event', split_by='number', frames_number=16)\n",
    "#     test_set = DVS128Gesture(args.data_dir, train=False, data_type='event', split_by='number', frames_number=16)\n",
    "\n",
    "#     train_data_loader = DataLoader(\n",
    "#         dataset=train_set,\n",
    "#         batch_size=16,\n",
    "#         shuffle=True,\n",
    "#         num_workers=4,\n",
    "#         drop_last=True,\n",
    "#         pin_memory=True)\n",
    "\n",
    "#     test_data_loader = DataLoader(\n",
    "#         dataset=test_set,\n",
    "#         batch_size=16,\n",
    "#         shuffle=False,\n",
    "#         num_workers=4,\n",
    "#         drop_last=False,\n",
    "#         pin_memory=True)\n",
    "\n",
    "#     scaler = amp.GradScaler()\n",
    "\n",
    "#     start_epoch = 0\n",
    "#     max_test_acc = 0\n",
    "    \n",
    "#     out_dir = os.path.join(args.out_dir, f'T_{16}_b_{16}_c_{128}_SGD_lr_{0.001}_CosALR_{32}_amp')\n",
    "    \n",
    "#     if not os.path.exists(out_dir):\n",
    "#         os.mkdir(out_dir)\n",
    "#         print(f'Mkdir {out_dir}.')\n",
    "        \n",
    "#     writer = SummaryWriter(os.path.join(out_dir, 'dvsg_logs'), purge_step=start_epoch)\n",
    "    \n",
    "#     for epoch in range(start_epoch, 64):\n",
    "#         start_time = time.time()\n",
    "#         net.train()\n",
    "#         train_loss = 0\n",
    "#         train_acc = 0\n",
    "#         train_samples = 0\n",
    "#         for frame, label in train_data_loader:\n",
    "#             optimizer.zero_grad()\n",
    "#             frame = frame.float().to('cuda:0')\n",
    "#             label = label.to('cuda:0')\n",
    "#             label_onehot = F.one_hot(label, 11).float()\n",
    "            \n",
    "#             with amp.autocast():\n",
    "#                 out_fr = net(frame)\n",
    "#                 loss = F.mse_loss(out_fr, label_onehot)\n",
    "#             scaler.scale(loss).backward()\n",
    "#             scaler.step(optimizer)\n",
    "#             scaler.update()\n",
    "\n",
    "#             train_samples += label.numel()\n",
    "#             train_loss += loss.item() * label.numel()\n",
    "#             train_acc += (out_fr.argmax(1) == label).float().sum().item()\n",
    "\n",
    "#             functional.reset_net(net)\n",
    "            \n",
    "#         train_loss /= train_samples\n",
    "#         train_acc /= train_samples\n",
    "\n",
    "#         writer.add_scalar('train_loss', train_loss, epoch)\n",
    "#         writer.add_scalar('train_acc', train_acc, epoch)\n",
    "#         lr_scheduler.step()\n",
    "\n",
    "#         net.eval()\n",
    "#         test_loss = 0\n",
    "#         test_acc = 0\n",
    "#         test_samples = 0\n",
    "#         with torch.no_grad():\n",
    "#             for frame, label in test_data_loader:\n",
    "#                 frame = frame.float().to('cuda:0')\n",
    "#                 label = label.to('cuda:0')\n",
    "#                 label_onehot = F.one_hot(lE:\\uni-stuff\\FYP\\gesture-recognition-snn\\datasets\\eventdataset.pyabel, 11).float()\n",
    "#                 out_fr = net(frame)\n",
    "#                 loss = F.mse_loss(out_fr, label_onehot)\n",
    "\n",
    "#                 test_samples += label.numel()\n",
    "#                 test_loss += loss.item() * label.numel()\n",
    "#                 test_acc += (out_fr.argmax(1) == label).float().sum().item()\n",
    "#                 functional.reset_net(net)\n",
    "\n",
    "#         test_loss /= test_samples\n",
    "#         test_acc /= test_samples\n",
    "\n",
    "#         writer.add_scalar('test_loss', test_loss, epoch)\n",
    "#         writer.add_scalar('test_acc', test_acc, epoch)\n",
    "        \n",
    "#         save_max = False\n",
    "#         if test_acc > max_test_acc:\n",
    "#             max_test_acc = test_acc\n",
    "#             save_max = True\n",
    "\n",
    "#         checkpoint = {\n",
    "#             'net': net.state_dict(),\n",
    "#             'optimizer': optimizer.state_dict(),\n",
    "#             'lr_scheduler': lr_scheduler.state_dict(),\n",
    "#             'epoch': epoch,\n",
    "#             'max_test_acc': max_test_acc\n",
    "#         }\n",
    "        \n",
    "#         if save_max:\n",
    "#             torch.save(checkpoint, os.path.join(out_dir, 'checkpoint_max.pth'))\n",
    "        \n",
    "#         torch.save(checkpoint, os.path.join(out_dir, 'checkpoint_max.pth'))\n",
    "\n",
    "#         print(f'epoch={epoch}, train_loss={train_loss}, train_acc={train_acc}, test_loss={test_loss}, test_acc={test_acc}, max_test_acc={max_test_acc}, total_time={time.time() - start_time}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f1ab7b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # python classify_dvsg.py -data_dir /userhome/datasets/DVS128Gesture -out_dir ./logs -amp -opt Adam -device cuda:0 -lr_scheduler CosALR -T_max 64 -cupy -epochs 1024\n",
    "    '''\n",
    "    * :ref:`API in English <classify_dvsg.__init__-en>`\n",
    "    .. _classify_dvsg.__init__-cn:\n",
    "    用于分类DVS128 Gesture数据集的代码样例。网络结构来自于 `Incorporating Learnable Membrane Time Constant to Enhance Learning of Spiking Neural Networks <https://arxiv.org/abs/2007.05785>`_。\n",
    "    .. code:: bash\n",
    "        usage: classify_dvsg.py [-h] [-T T] [-device DEVICE] [-b B] [-epochs N] [-j N] [-channels CHANNELS] [-data_dir DATA_DIR] [-out_dir OUT_DIR] [-resume RESUME] [-amp] [-cupy] [-opt OPT] [-lr LR] [-momentum MOMENTUM] [-lr_scheduler LR_SCHEDULER] [-step_size STEP_SIZE] [-gamma GAMMA] [-T_max T_MAX]\n",
    "        Classify DVS128 Gesture\n",
    "        optional arguments:\n",
    "          -h, --help            show this help message and exit\n",
    "          -T T                  simulating time-steps\n",
    "          -device DEVICE        device\n",
    "          -b B                  batch size\n",
    "          -epochs N             number of total epochs to run\n",
    "          -j N                  number of data loading workers (default: 4)\n",
    "          -channels CHANNELS    channels of Conv2d in SNN\n",
    "          -data_dir DATA_DIR    root dir of DVS128 Gesture dataset\n",
    "          -out_dir OUT_DIR      root dir for saving logs and checkpoint\n",
    "          -resume RESUME        resume from the checkpoint path\n",
    "          -amp                  automatic mixed precision training\n",
    "          -cupy                 use CUDA neuron and multi-step forward mode\n",
    "          -opt OPT              use which optimizer. SDG or Adam\n",
    "          -lr LR                learning rate\n",
    "          -momentum MOMENTUM    momentum for SGD\n",
    "          -lr_scheduler LR_SCHEDULER\n",
    "                                use which schedule. StepLR or CosALR\n",
    "          -step_size STEP_SIZE  step_size for StepLR\n",
    "          -gamma GAMMA          gamma for StepLR\n",
    "          -T_max T_MAX          T_max for CosineAnnealingLR\n",
    "    运行示例：\n",
    "    .. code:: bash\n",
    "        python -m spikingjelly.clock_driven.examples.classify_dvsg -data_dir /userhome/datasets/DVS128Gesture -out_dir ./logs -amp -opt Adam -device cuda:0 -lr_scheduler CosALR -T_max 64 -cupy -epochs 1024\n",
    "    阅读教程 :doc:`./clock_driven/14_classify_dvsg` 以获得更多信息。\n",
    "    * :ref:`中文API <classify_dvsg.__init__-cn>`\n",
    "    .. _classify_dvsg.__init__-en:\n",
    "    The code example for classifying the DVS128 Gesture dataset. The network structure is from `Incorporating Learnable Membrane Time Constant to Enhance Learning of Spiking Neural Networks <https://arxiv.org/abs/2007.05785>`_.\n",
    "    .. code:: bash\n",
    "        usage: classify_dvsg.py [-h] [-T T] [-device DEVICE] [-b B] [-epochs N] [-j N] [-channels CHANNELS] [-data_dir DATA_DIR] [-out_dir OUT_DIR] [-resume RESUME] [-amp] [-cupy] [-opt OPT] [-lr LR] [-momentum MOMENTUM] [-lr_scheduler LR_SCHEDULER] [-step_size STEP_SIZE] [-gamma GAMMA] [-T_max T_MAX]\n",
    "        Classify DVS128 Gesture\n",
    "        optional arguments:\n",
    "          -h, --help            show this help message and exit\n",
    "          -T T                  simulating time-steps\n",
    "          -device DEVICE        device\n",
    "          -b B                  batch size\n",
    "          -epochs N             number of total epochs to run\n",
    "          -j N                  number of data loading workers (default: 4)\n",
    "          -channels CHANNELS    channels of Conv2d in SNN\n",
    "          -data_dir DATA_DIR    root dir of DVS128 Gesture dataset\n",
    "          -out_dir OUT_DIR      root dir for saving logs and checkpoint\n",
    "          -resume RESUME        resume from the checkpoint path\n",
    "          -amp                  automatic mixed precision training\n",
    "          -cupy                 use CUDA neuron and multi-step forward mode\n",
    "          -opt OPT              use which optimizer. SDG or Adam\n",
    "          -lr LR                learning rate\n",
    "          -momentum MOMENTUM    momentum for SGD\n",
    "          -lr_scheduler LR_SCHEDULER\n",
    "                                use which schedule. StepLR or CosALR\n",
    "          -step_size STEP_SIZE  step_size for StepLR\n",
    "          -gamma GAMMA          gamma for StepLR\n",
    "          -T_max T_MAX          T_max for CosineAnnealingLR\n",
    "    Running Example:\n",
    "    .. code:: bash\n",
    "        python -m spikingjelly.clock_driven.examples.classify_dvsg -data_dir /userhome/datasets/DVS128Gesture -out_dir ./logs -amp -opt Adam -device cuda:0 -lr_scheduler CosALR -T_max 64 -cupy -epochs 1024\n",
    "    See the tutorial :doc:`./clock_driven_en/14_classify_dvsg` for more details.\n",
    "    '''\n",
    "    parser = argparse.ArgumentParser(description='Classify DVS128 Gesture')\n",
    "    parser.add_argument('-T', default=16, type=int, help='simulating time-steps')\n",
    "    parser.add_argument('-device', default='cuda:0', help='device')\n",
    "    parser.add_argument('-b', default=4, type=int, help='batch size')\n",
    "    parser.add_argument('-epochs', default=64, type=int, metavar='N',\n",
    "                        help='number of total epochs to run')\n",
    "    parser.add_argument('-j', default=4, type=int, metavar='N',\n",
    "                        help='number of data loading workers (default: 4)')\n",
    "    parser.add_argument('-channels', default=128, type=int, help='channels of Conv2d in SNN')\n",
    "    parser.add_argument('-data_dir', type=str, default='./data', help='root dir')\n",
    "    parser.add_argument('-out_dir', type=str, default='./output', help='root dir for saving logs and checkpoint')\n",
    "\n",
    "#     parser.add_argument('-resume', type=str, help='resume from the checkpoint path')\n",
    "#     parser.add_argument('-amp', action='store_true', help='automatic mixed precision training')\n",
    "#     parser.add_argument('-cupy', action='store_true', help='use CUDA neuron and multi-step forward mode')\n",
    "\n",
    "\n",
    "#     parser.add_argument('-opt', type=str, help='use which optimizer. SDG or Adam')\n",
    "    parser.add_argument('-lr', default=0.001, type=float, help='learning rate')\n",
    "    parser.add_argument('-momentum', default=0.9, type=float, help='momentum for SGD')\n",
    "    parser.add_argument('-lr_scheduler', default='CosALR', type=str, help='use which schedule. StepLR or CosALR')\n",
    "    parser.add_argument('-step_size', default=32, type=float, help='step_size for StepLR')\n",
    "    parser.add_argument('-gamma', default=0.1, type=float, help='gamma for StepLR')\n",
    "    parser.add_argument('-T_max', default=32, type=int, help='T_max for CosineAnnealingLR')\n",
    "\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    print(args)\n",
    "\n",
    "    net = PythonNet(channels=args.channels)\n",
    "    print(net)\n",
    "    net.to(args.device)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#     optimizer = None\n",
    "#     if args.opt == 'SGD':\n",
    "    optimizer = torch.optim.SGD(net.parameters(), lr=args.lr, momentum=args.momentum)\n",
    "#     elif args.opt == 'Adam':\n",
    "#         optimizer = torch.optim.Adam(net.parameters(), lr=args.lr)\n",
    "#     else:\n",
    "#         raise NotImplementedError(args.opt)\n",
    "\n",
    "#     lr_scheduler = None\n",
    "#     if args.lr_scheduler == 'StepLR':\n",
    "#         lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=args.step_size, gamma=args.gamma)\n",
    "#     elif args.lr_scheduler == 'CosALR':\n",
    "    lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=args.T_max)\n",
    "#     else:\n",
    "#         raise NotImplementedError(args.lr_scheduler)\n",
    "\n",
    "    train_set = DVS128Gesture(args.data_dir, train=True, data_type='frame', split_by='number', frames_number=args.T)\n",
    "    test_set = DVS128Gesture(args.data_dir, train=False, data_type='frame', split_by='number', frames_number=args.T)\n",
    "\n",
    "    train_data_loader = DataLoader(\n",
    "        dataset=train_set,\n",
    "        batch_size=args.b,\n",
    "        shuffle=True,\n",
    "        num_workers=args.j,\n",
    "        drop_last=True,\n",
    "        pin_memory=True)\n",
    "\n",
    "    test_data_loader = DataLoader(\n",
    "        dataset=test_set,\n",
    "        batch_size=args.b,\n",
    "        shuffle=False,\n",
    "        num_workers=args.j,\n",
    "        drop_last=False,\n",
    "        pin_memory=True)\n",
    "\n",
    "#     scaler = None\n",
    "#     if args.amp:\n",
    "    scaler = amp.GradScaler()\n",
    "\n",
    "    start_epoch = 0\n",
    "    max_test_acc = 0\n",
    "\n",
    "#     if args.resume:\n",
    "#         checkpoint = torch.load(args.resume, map_location='cpu')\n",
    "#         net.load_state_dict(checkpoint['net'])\n",
    "#         optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "#         lr_scheduler.load_state_dict(checkpoint['lr_scheduler'])\n",
    "#         start_epoch = checkpoint['epoch'] + 1\n",
    "#         max_test_acc = checkpoint['max_test_acc']\n",
    "\n",
    "    out_dir = os.path.join(args.out_dir, f'T_{args.T}_b_{args.b}_c_{args.channels}_{args.opt}_lr_{args.lr}_')\n",
    "#     if args.lr_scheduler == 'CosALR':\n",
    "    out_dir += f'CosALR_{args.T_max}'\n",
    "#     elif args.lr_scheduler == 'StepLR':\n",
    "#         out_dir += f'StepLR_{args.step_size}_{args.gamma}'\n",
    "#     else:\n",
    "#         raise NotImplementedError(args.lr_scheduler)\n",
    "\n",
    "#     if args.amp:\n",
    "    out_dir += '_amp'\n",
    "#     if args.cupy:\n",
    "#         out_dir += '_cupy'\n",
    "\n",
    "\n",
    "    if not os.path.exists(out_dir):\n",
    "        os.mkdir(out_dir)\n",
    "        print(f'Mkdir {out_dir}.')\n",
    "\n",
    "    with open(os.path.join(out_dir, 'args.txt'), 'w', encoding='utf-8') as args_txt:\n",
    "        args_txt.write(str(args))\n",
    "\n",
    "    writer = SummaryWriter(os.path.join(out_dir, 'dvsg_logs'), purge_step=start_epoch)\n",
    "\n",
    "    for epoch in range(start_epoch, args.epochs):\n",
    "        start_time = time.time()\n",
    "        net.train()\n",
    "        train_loss = 0\n",
    "        train_acc = 0\n",
    "        train_samples = 0\n",
    "        for frame, label in train_data_loader:\n",
    "            optimizer.zero_grad()\n",
    "            frame = frame.float().to(args.device)\n",
    "            label = label.to(args.device)\n",
    "            label_onehot = F.one_hot(label, 11).float()\n",
    "            if args.amp:\n",
    "                with amp.autocast():\n",
    "                    out_fr = net(frame)\n",
    "                    loss = F.mse_loss(out_fr, label_onehot)\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                out_fr = net(frame)\n",
    "                loss = F.mse_loss(out_fr, label_onehot)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            train_samples += label.numel()\n",
    "            train_loss += loss.item() * label.numel()\n",
    "            train_acc += (out_fr.argmax(1) == label).float().sum().item()\n",
    "\n",
    "            functional.reset_net(net)\n",
    "        train_loss /= train_samples\n",
    "        train_acc /= train_samples\n",
    "\n",
    "        writer.add_scalar('train_loss', train_loss, epoch)\n",
    "        writer.add_scalar('train_acc', train_acc, epoch)\n",
    "        lr_scheduler.step()\n",
    "\n",
    "        net.eval()\n",
    "        test_loss = 0\n",
    "        test_acc = 0\n",
    "        test_samples = 0\n",
    "        with torch.no_grad():\n",
    "            for frame, label in test_data_loader:\n",
    "                frame = frame.float().to(args.device)\n",
    "                label = label.to(args.device)\n",
    "                label_onehot = F.one_hot(label, 11).float()\n",
    "                out_fr = net(frame)\n",
    "                loss = F.mse_loss(out_fr, label_onehot)\n",
    "\n",
    "                test_samples += label.numel()\n",
    "                test_loss += loss.item() * label.numel()\n",
    "                test_acc += (out_fr.argmax(1) == label).float().sum().item()\n",
    "                functional.reset_net(net)\n",
    "\n",
    "        test_loss /= test_samples\n",
    "        test_acc /= test_samples\n",
    "        writer.add_scalar('test_loss', test_loss, epoch)\n",
    "        writer.add_scalar('test_acc', test_acc, epoch)\n",
    "\n",
    "        save_max = False\n",
    "        if test_acc > max_test_acc:\n",
    "            max_test_acc = test_acc\n",
    "            save_max = True\n",
    "\n",
    "        checkpoint = {\n",
    "            'net': net.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'lr_scheduler': lr_scheduler.state_dict(),\n",
    "            'epoch': epoch,\n",
    "            'max_test_acc': max_test_acc\n",
    "        }\n",
    "\n",
    "        if save_max:\n",
    "            torch.save(checkpoint, os.path.join(out_dir, 'checkpoint_max.pth'))\n",
    "\n",
    "        torch.save(checkpoint, os.path.join(out_dir, 'checkpoint_latest.pth'))\n",
    "\n",
    "        print(args)\n",
    "        print(f'epoch={epoch}, train_loss={train_loss}, train_acc={train_acc}, test_loss={test_loss}, test_acc={test_acc}, max_test_acc={max_test_acc}, total_time={time.time() - start_time}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9ad47a30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(T=16, T_max=32, b=4, channels=128, data_dir='./data', device='cuda:0', epochs=64, gamma=0.1, j=4, lr=0.001, lr_scheduler='CosALR', momentum=0.9, out_dir='./output', step_size=32)\n",
      "PythonNet(\n",
      "  (conv): Sequential(\n",
      "    (0): Conv2d(2, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): LIFNode(\n",
      "      v_threshold=1.0, v_reset=0.0, detach_reset=True, tau=2.0\n",
      "      (surrogate_function): ATan(alpha=2.0, spiking=True)\n",
      "    )\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (4): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): LIFNode(\n",
      "      v_threshold=1.0, v_reset=0.0, detach_reset=True, tau=2.0\n",
      "      (surrogate_function): ATan(alpha=2.0, spiking=True)\n",
      "    )\n",
      "    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (8): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (9): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (10): LIFNode(\n",
      "      v_threshold=1.0, v_reset=0.0, detach_reset=True, tau=2.0\n",
      "      (surrogate_function): ATan(alpha=2.0, spiking=True)\n",
      "    )\n",
      "    (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (12): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (13): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (14): LIFNode(\n",
      "      v_threshold=1.0, v_reset=0.0, detach_reset=True, tau=2.0\n",
      "      (surrogate_function): ATan(alpha=2.0, spiking=True)\n",
      "    )\n",
      "    (15): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (16): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (17): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (18): LIFNode(\n",
      "      v_threshold=1.0, v_reset=0.0, detach_reset=True, tau=2.0\n",
      "      (surrogate_function): ATan(alpha=2.0, spiking=True)\n",
      "    )\n",
      "    (19): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (fc): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1)\n",
      "    (1): Dropout(p=0.5)\n",
      "    (2): Linear(in_features=2048, out_features=512, bias=False)\n",
      "    (3): LIFNode(\n",
      "      v_threshold=1.0, v_reset=0.0, detach_reset=True, tau=2.0\n",
      "      (surrogate_function): ATan(alpha=2.0, spiking=True)\n",
      "    )\n",
      "    (4): Dropout(p=0.5)\n",
      "    (5): Linear(in_features=512, out_features=110, bias=False)\n",
      "    (6): LIFNode(\n",
      "      v_threshold=1.0, v_reset=0.0, detach_reset=True, tau=2.0\n",
      "      (surrogate_function): ATan(alpha=2.0, spiking=True)\n",
      "    )\n",
      "  )\n",
      "  (vote): VotingLayer(\n",
      "    (voting): AvgPool1d(kernel_size=(10,), stride=(10,), padding=(0,))\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [12]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m----> 2\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [8]\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     96\u001b[0m     net \u001b[38;5;241m=\u001b[39m PythonNet(channels\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mchannels)\n\u001b[0;32m     97\u001b[0m     \u001b[38;5;28mprint\u001b[39m(net)\n\u001b[1;32m---> 98\u001b[0m     \u001b[43mnet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;66;03m#     optimizer = None\u001b[39;00m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;66;03m#     if args.opt == 'SGD':\u001b[39;00m\n\u001b[0;32m    105\u001b[0m     optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mSGD(net\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mlr, momentum\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mmomentum)\n",
      "File \u001b[1;32mE:\\uni-stuff\\FYP\\gesture-recognition-snn\\snnenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:899\u001b[0m, in \u001b[0;36mModule.to\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    895\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    896\u001b[0m                     non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[0;32m    897\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, non_blocking)\n\u001b[1;32m--> 899\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mE:\\uni-stuff\\FYP\\gesture-recognition-snn\\snnenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:570\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    568\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[0;32m    569\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 570\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    572\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    573\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    574\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    575\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    580\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    581\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32mE:\\uni-stuff\\FYP\\gesture-recognition-snn\\snnenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:570\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    568\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[0;32m    569\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 570\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    572\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    573\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    574\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    575\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    580\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    581\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32mE:\\uni-stuff\\FYP\\gesture-recognition-snn\\snnenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:593\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    589\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[0;32m    590\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[0;32m    591\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[0;32m    592\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m--> 593\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    594\u001b[0m should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[0;32m    595\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[1;32mE:\\uni-stuff\\FYP\\gesture-recognition-snn\\snnenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:897\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m    894\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[0;32m    895\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    896\u001b[0m                 non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[1;32m--> 897\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mE:\\uni-stuff\\FYP\\gesture-recognition-snn\\snnenv\\lib\\site-packages\\torch\\cuda\\__init__.py:208\u001b[0m, in \u001b[0;36m_lazy_init\u001b[1;34m()\u001b[0m\n\u001b[0;32m    204\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    205\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    206\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultiprocessing, you must use the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspawn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m start method\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(torch\u001b[38;5;241m.\u001b[39m_C, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_cuda_getDeviceCount\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m--> 208\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTorch not compiled with CUDA enabled\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    209\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _cudart \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    210\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[0;32m    211\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "snnenv",
   "language": "python",
   "name": "snnenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
